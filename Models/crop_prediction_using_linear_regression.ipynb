{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.10 64-bit ('pytor1.12': conda)",
   "display_name": "Python 3.6.10 64-bit ('pytor1.12': conda)",
   "metadata": {
    "interpreter": {
     "hash": "521b0883aadebb34975bad4f964c25c8a8224771a5b887cc0070c3c89f50aff7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file\n",
    "df = pd.read_csv('../dataset/crops_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('Harvest Temp', axis=1,inplace=True)\n",
    "# converting from pandas dataframe to numpy\n",
    "np_inputs = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[32.619118   6.8041816 63.141884  ...  4.5212717 14.998593  12.280917 ]\n",
      " [30.113255   7.360165  61.930634  ...  4.54938   15.128822  12.31593  ]\n",
      " [32.86468    7.3836966 62.03019   ...  4.5945315 14.124709  12.205821 ]\n",
      " ...\n",
      " [21.732143   6.63041   42.23988   ... 51.347816  29.18824   12.777757 ]\n",
      " [22.688066   6.362192  35.849293  ... 48.710094  27.44639   12.33395  ]\n",
      " [21.025482   6.1696644 70.2149    ... 50.24635   26.575783  10.986518 ]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# inputs are from col-6 to col-13\n",
    "inputs = np_inputs[:, 6:14]\n",
    "inputs = np.array(inputs, dtype='float32')\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "\n",
    "\n",
    "# convert output crops to binary encoded labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np_inputs[:,14])\n",
    "outputs = lb.transform(np_inputs[:,14])\n",
    "\n",
    "outputs = np.array(outputs, dtype='float32')\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[32.6191,  6.8042, 63.1419,  ...,  4.5213, 14.9986, 12.2809],\n        [30.1133,  7.3602, 61.9306,  ...,  4.5494, 15.1288, 12.3159],\n        [32.8647,  7.3837, 62.0302,  ...,  4.5945, 14.1247, 12.2058],\n        ...,\n        [21.7321,  6.6304, 42.2399,  ..., 51.3478, 29.1882, 12.7778],\n        [22.6881,  6.3622, 35.8493,  ..., 48.7101, 27.4464, 12.3340],\n        [21.0255,  6.1697, 70.2149,  ..., 50.2463, 26.5758, 10.9865]])\ntensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [1., 0., 0.,  ..., 0., 0., 0.],\n        [1., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 1.],\n        [0., 0., 0.,  ..., 0., 0., 1.],\n        [0., 0., 0.,  ..., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# converting numpy array into torch tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "outputs = torch.from_numpy(outputs)\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "121081\n(tensor([[32.6191,  6.8042, 63.1419, 31.8323, 36.0894,  4.5213, 14.9986, 12.2809],\n        [30.1133,  7.3602, 61.9306, 31.7180, 39.6473,  4.5494, 15.1288, 12.3159],\n        [32.8647,  7.3837, 62.0302, 31.8742, 35.8610,  4.5945, 14.1247, 12.2058]]), tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "ds = TensorDataset(inputs,outputs)\n",
    "print(len(ds))\n",
    "print(ds[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(96865, 24216)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "train_ds, valid_ds = random_split(ds, [96865, 24216])\n",
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1514\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds , batch_size , shuffle=True )\n",
    "val_loader = DataLoader(valid_ds , batch_size)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# defining neural network parameteres\n",
    "input_nodes = 8\n",
    "hidden1_nodes = 128\n",
    "hidden2_nodes = 128\n",
    "output_nodes = 29\n",
    "\n",
    "# creating neural net\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Linear(input_nodes, hidden1_nodes)\n",
    "        self.hidden1 = nn.Linear(hidden1_nodes, hidden2_nodes)\n",
    "        self.hidden2 = nn.Linear(hidden2_nodes, output_nodes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2581,  0.0027,  0.1920,  ..., -0.1665,  0.3195,  0.1416],\n",
       "         [-0.1419, -0.3445, -0.1335,  ..., -0.1858,  0.3412,  0.1519],\n",
       "         [ 0.2388, -0.3131, -0.1929,  ...,  0.0464, -0.0354,  0.2755],\n",
       "         ...,\n",
       "         [ 0.1397,  0.0094, -0.3507,  ..., -0.1579,  0.3159,  0.1161],\n",
       "         [-0.1624, -0.1901, -0.0557,  ..., -0.3073,  0.0459, -0.1414],\n",
       "         [ 0.3040, -0.1529,  0.0737,  ...,  0.2312, -0.0967,  0.0335]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2892, -0.0811,  0.2834,  0.0526,  0.0546,  0.1613, -0.0774,  0.1764,\n",
       "          0.1001, -0.2384, -0.1137, -0.3087, -0.2936,  0.1555,  0.2477,  0.2961,\n",
       "         -0.2343, -0.2061,  0.1745,  0.0784,  0.1614, -0.2926, -0.2382,  0.1563,\n",
       "         -0.2599,  0.1028,  0.0105,  0.0400,  0.0262, -0.2588,  0.0467, -0.1171,\n",
       "         -0.3025, -0.0097,  0.2909,  0.1575, -0.3161, -0.1067, -0.0573,  0.0724,\n",
       "         -0.1499,  0.1918,  0.3148,  0.3335, -0.3222,  0.2985, -0.0840, -0.2297,\n",
       "         -0.0380,  0.2639,  0.3110, -0.1256, -0.0300,  0.1112,  0.0136, -0.0009,\n",
       "         -0.0265,  0.2660, -0.3115, -0.2084, -0.0264, -0.0589, -0.1047,  0.2058,\n",
       "          0.0510,  0.0423, -0.2223,  0.2751,  0.1598,  0.2059,  0.3309, -0.3280,\n",
       "          0.1773, -0.0971,  0.1155, -0.0935, -0.1307, -0.2459, -0.3093, -0.0888,\n",
       "          0.2968, -0.3046, -0.0152,  0.2138, -0.3424, -0.0091,  0.2002,  0.2990,\n",
       "         -0.0542, -0.2337,  0.1172, -0.1048, -0.1469,  0.1462, -0.2237,  0.0028,\n",
       "         -0.2801,  0.1438, -0.2376, -0.1060, -0.1865,  0.2267, -0.3441,  0.3161,\n",
       "          0.3198,  0.0322,  0.2486,  0.2151, -0.1289,  0.1352, -0.3476,  0.0381,\n",
       "         -0.0105,  0.0337,  0.1017,  0.3003,  0.0352,  0.0916,  0.0943, -0.1818,\n",
       "          0.1751, -0.2928, -0.3217, -0.3517, -0.1702, -0.3478,  0.2865,  0.3347],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0522, -0.0331,  0.0725,  ..., -0.0330, -0.0284, -0.0829],\n",
       "         [-0.0607,  0.0022, -0.0154,  ...,  0.0103,  0.0165,  0.0486],\n",
       "         [-0.0109,  0.0131, -0.0254,  ...,  0.0075, -0.0653,  0.0870],\n",
       "         ...,\n",
       "         [-0.0503, -0.0088, -0.0056,  ..., -0.0884,  0.0607,  0.0315],\n",
       "         [ 0.0345,  0.0818,  0.0096,  ..., -0.0492,  0.0786,  0.0442],\n",
       "         [ 0.0159,  0.0157, -0.0726,  ..., -0.0296,  0.0751,  0.0046]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0612,  0.0138,  0.0322, -0.0881,  0.0593,  0.0866, -0.0617,  0.0287,\n",
       "          0.0082,  0.0613,  0.0730, -0.0785,  0.0304, -0.0356, -0.0561, -0.0082,\n",
       "          0.0070,  0.0124,  0.0570, -0.0594, -0.0395, -0.0213,  0.0487, -0.0055,\n",
       "         -0.0231,  0.0704, -0.0022,  0.0120, -0.0672,  0.0725, -0.0530, -0.0251,\n",
       "         -0.0431, -0.0665, -0.0563, -0.0612,  0.0302,  0.0065, -0.0652,  0.0475,\n",
       "          0.0587,  0.0315, -0.0177,  0.0759, -0.0563,  0.0762,  0.0066, -0.0297,\n",
       "         -0.0169,  0.0583, -0.0661, -0.0807,  0.0312,  0.0532,  0.0778,  0.0431,\n",
       "          0.0528,  0.0118, -0.0665,  0.0346, -0.0826,  0.0624,  0.0070, -0.0152,\n",
       "          0.0413, -0.0770,  0.0110, -0.0848,  0.0012,  0.0170, -0.0640, -0.0227,\n",
       "         -0.0831, -0.0601,  0.0074, -0.0363, -0.0616, -0.0759, -0.0415,  0.0121,\n",
       "         -0.0800, -0.0456, -0.0532, -0.0216, -0.0020,  0.0008,  0.0021,  0.0529,\n",
       "          0.0869, -0.0729, -0.0795,  0.0780,  0.0478,  0.0103,  0.0251,  0.0859,\n",
       "         -0.0266, -0.0755,  0.0372, -0.0517,  0.0719,  0.0450,  0.0076,  0.0838,\n",
       "          0.0576, -0.0162, -0.0497,  0.0363, -0.0012, -0.0552,  0.0224,  0.0567,\n",
       "          0.0858, -0.0107,  0.0023, -0.0651, -0.0571, -0.0376,  0.0661,  0.0205,\n",
       "         -0.0692,  0.0859,  0.0736, -0.0198, -0.0781, -0.0741, -0.0185, -0.0216],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0705, -0.0705, -0.0700,  ..., -0.0695,  0.0438,  0.0393],\n",
       "         [-0.0348, -0.0138, -0.0575,  ...,  0.0816,  0.0308,  0.0261],\n",
       "         [ 0.0853,  0.0262, -0.0128,  ..., -0.0107,  0.0042,  0.0872],\n",
       "         ...,\n",
       "         [-0.0796,  0.0776, -0.0707,  ...,  0.0865, -0.0548,  0.0369],\n",
       "         [ 0.0854,  0.0181, -0.0400,  ...,  0.0643, -0.0705, -0.0381],\n",
       "         [ 0.0665,  0.0290,  0.0372,  ..., -0.0112,  0.0600, -0.0232]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0437,  0.0100,  0.0711,  0.0487, -0.0862,  0.0518,  0.0150, -0.0357,\n",
       "          0.0033, -0.0655,  0.0647, -0.0823, -0.0816, -0.0609,  0.0422,  0.0114,\n",
       "          0.0360, -0.0637, -0.0167, -0.0141, -0.0196,  0.0304, -0.0857,  0.0333,\n",
       "          0.0195, -0.0708,  0.0281, -0.0668,  0.0334], requires_grad=True)]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "model = Model()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_dict={}\n",
    "def train(model,epochs,train_batch,valid_batch,lr,opt_fn=torch.optim.SGD):\n",
    "    opt = opt_fn(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        loss_dict[epoch] = 0\n",
    "        i = 0\n",
    "        for input_part, output_part in train_batch:\n",
    "            i+=1\n",
    "            output = model(input_part)\n",
    "            loss = F.mse_loss(output,output_part)\n",
    "            loss_dict[epoch]+=loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            #print(\"Done with {0} part of {1}/{2}\".format(i,epoch,epochs))\n",
    "        loss_dict[epoch]/1514\n",
    "        print(\"For epoch {0} avg_loss = {1}\".format(epoch,loss_dict[epoch]))\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For epoch 0 avg_loss = 52.91108322143555\n",
      "For epoch 1 avg_loss = 51.06782150268555\n",
      "For epoch 2 avg_loss = 50.69636535644531\n",
      "For epoch 3 avg_loss = 50.584373474121094\n",
      "For epoch 4 avg_loss = 50.51202392578125\n",
      "For epoch 5 avg_loss = 50.450504302978516\n",
      "For epoch 6 avg_loss = 50.397247314453125\n",
      "For epoch 7 avg_loss = 50.353981018066406\n",
      "For epoch 8 avg_loss = 50.316837310791016\n",
      "For epoch 9 avg_loss = 50.286006927490234\n",
      "For epoch 10 avg_loss = 50.25830841064453\n",
      "For epoch 11 avg_loss = 50.22936248779297\n",
      "For epoch 12 avg_loss = 50.197566986083984\n",
      "For epoch 13 avg_loss = 50.16925811767578\n",
      "For epoch 14 avg_loss = 50.13800048828125\n",
      "For epoch 15 avg_loss = 50.10002136230469\n",
      "For epoch 16 avg_loss = 50.04446029663086\n",
      "For epoch 17 avg_loss = 50.002098083496094\n",
      "For epoch 18 avg_loss = 49.97929763793945\n",
      "For epoch 19 avg_loss = 49.962772369384766\n",
      "For epoch 20 avg_loss = 49.94807052612305\n",
      "For epoch 21 avg_loss = 49.93580627441406\n",
      "For epoch 22 avg_loss = 49.92451477050781\n",
      "For epoch 23 avg_loss = 49.91460418701172\n",
      "For epoch 24 avg_loss = 49.90518569946289\n",
      "For epoch 25 avg_loss = 49.89506912231445\n",
      "For epoch 26 avg_loss = 49.88669204711914\n",
      "For epoch 27 avg_loss = 49.877586364746094\n",
      "For epoch 28 avg_loss = 49.86954116821289\n",
      "For epoch 29 avg_loss = 49.86251449584961\n",
      "For epoch 30 avg_loss = 49.85456848144531\n",
      "For epoch 31 avg_loss = 49.84756088256836\n",
      "For epoch 32 avg_loss = 49.84111404418945\n",
      "For epoch 33 avg_loss = 49.83365249633789\n",
      "For epoch 34 avg_loss = 49.82719421386719\n",
      "For epoch 35 avg_loss = 49.8217887878418\n",
      "For epoch 36 avg_loss = 49.81535339355469\n",
      "For epoch 37 avg_loss = 49.80992126464844\n",
      "For epoch 38 avg_loss = 49.804290771484375\n",
      "For epoch 39 avg_loss = 49.79827117919922\n",
      "For epoch 40 avg_loss = 49.7918701171875\n",
      "For epoch 41 avg_loss = 49.7872428894043\n",
      "For epoch 42 avg_loss = 49.7819709777832\n",
      "For epoch 43 avg_loss = 49.77615737915039\n",
      "For epoch 44 avg_loss = 49.77043533325195\n",
      "For epoch 45 avg_loss = 49.765037536621094\n",
      "For epoch 46 avg_loss = 49.76000213623047\n",
      "For epoch 47 avg_loss = 49.75526809692383\n",
      "For epoch 48 avg_loss = 49.74953842163086\n",
      "For epoch 49 avg_loss = 49.74347686767578\n",
      "For epoch 50 avg_loss = 49.737979888916016\n",
      "For epoch 51 avg_loss = 49.73284149169922\n",
      "For epoch 52 avg_loss = 49.72684097290039\n",
      "For epoch 53 avg_loss = 49.72200012207031\n",
      "For epoch 54 avg_loss = 49.71630859375\n",
      "For epoch 55 avg_loss = 49.711483001708984\n",
      "For epoch 56 avg_loss = 49.704959869384766\n",
      "For epoch 57 avg_loss = 49.69997024536133\n",
      "For epoch 58 avg_loss = 49.693946838378906\n",
      "For epoch 59 avg_loss = 49.68830490112305\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "lr = 1e-2\n",
    "\n",
    "history = train(model, epochs, train_loader, val_loader, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T17:45:20.107797</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 378.465625 248.518125 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \nL 371.265625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"maa1d18c7c4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#maa1d18c7c4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.270864\" xlink:href=\"#maa1d18c7c4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(96.908364 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.857921\" xlink:href=\"#maa1d18c7c4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(148.495421 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"206.444978\" xlink:href=\"#maa1d18c7c4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(200.082478 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"258.032035\" xlink:href=\"#maa1d18c7c4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(251.669535 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.619092\" xlink:href=\"#maa1d18c7c4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(303.256592 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.206149\" xlink:href=\"#maa1d18c7c4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g transform=\"translate(354.843649 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mf92bd7e178\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf92bd7e178\" y=\"195.638195\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 50.0 -->\n      <g transform=\"translate(7.2 199.437413)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf92bd7e178\" y=\"164.970133\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 50.5 -->\n      <g transform=\"translate(7.2 168.769352)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf92bd7e178\" y=\"134.302072\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 51.0 -->\n      <g transform=\"translate(7.2 138.101291)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf92bd7e178\" y=\"103.63401\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 51.5 -->\n      <g transform=\"translate(7.2 107.433229)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf92bd7e178\" y=\"72.965949\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 52.0 -->\n      <g transform=\"translate(7.2 76.765168)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf92bd7e178\" y=\"42.297887\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 52.5 -->\n      <g transform=\"translate(7.2 46.097106)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf92bd7e178\" y=\"11.629826\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 53.0 -->\n      <g transform=\"translate(7.2 15.429045)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pc20ff4746d)\" d=\"M 51.683807 17.083636 \nL 56.842513 130.142164 \nL 62.001218 152.925844 \nL 67.159924 159.794991 \nL 72.31863 164.232632 \nL 77.477335 168.006007 \nL 82.636041 171.272585 \nL 87.794747 173.926371 \nL 92.953452 176.204622 \nL 98.112158 178.095639 \nL 103.270864 179.794558 \nL 108.42957 181.569989 \nL 113.588275 183.520202 \nL 118.746981 185.256558 \nL 123.905687 187.17378 \nL 129.064392 189.503272 \nL 134.223098 192.911173 \nL 139.381804 195.509506 \nL 144.540509 196.907997 \nL 149.699215 197.921593 \nL 154.857921 198.823347 \nL 160.016627 199.575589 \nL 165.175332 200.268166 \nL 170.334038 200.876043 \nL 175.492744 201.453736 \nL 180.651449 202.074248 \nL 185.810155 202.588065 \nL 190.968861 203.146573 \nL 196.127566 203.640034 \nL 201.286272 204.071023 \nL 206.444978 204.5584 \nL 211.603684 204.988219 \nL 216.762389 205.383643 \nL 221.921095 205.841306 \nL 227.079801 206.237432 \nL 232.238506 206.56898 \nL 237.397212 206.963702 \nL 242.555918 207.296887 \nL 247.714623 207.64224 \nL 252.873329 208.011458 \nL 258.032035 208.404075 \nL 263.190741 208.687891 \nL 268.349446 209.01125 \nL 273.508152 209.367833 \nL 278.666858 209.718801 \nL 283.825563 210.049881 \nL 288.984269 210.358733 \nL 294.142975 210.649101 \nL 299.30168 211.000537 \nL 304.460386 211.372329 \nL 309.619092 211.709492 \nL 314.777798 212.024662 \nL 319.936503 212.39271 \nL 325.095209 212.689629 \nL 330.253915 213.038726 \nL 335.41262 213.334709 \nL 340.571326 213.734812 \nL 345.730032 214.040857 \nL 350.888737 214.410309 \nL 356.047443 214.756364 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 371.265625 224.64 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc20ff4746d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(list(loss_dict.keys()), list(loss_dict.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'crop_prediction_weights_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('input.weight',\n",
       "              tensor([[-3.2542e-01, -3.1018e-01,  3.3977e-01, -3.2862e-02, -1.9732e-01,\n",
       "                       -1.1428e-01,  1.1619e-01,  2.8608e-01],\n",
       "                      [ 3.5996e-01,  1.3895e-01, -1.4563e-01,  3.6921e-01, -2.2927e-01,\n",
       "                        2.1933e-01, -4.0005e-01,  2.4126e-01],\n",
       "                      [-1.8391e-01,  2.8096e-01,  1.1709e-02, -2.3658e-01,  2.7456e-01,\n",
       "                        2.9811e-01, -3.5366e-01,  3.1016e-01],\n",
       "                      [-2.3667e-01, -3.0875e-01,  2.5648e-01, -1.0824e-01,  1.2827e-01,\n",
       "                        1.4139e-01,  4.6707e-02, -4.7841e-01],\n",
       "                      [-1.5888e-01, -1.0514e-01, -1.3736e-01, -2.9876e-01,  2.3532e-01,\n",
       "                       -6.7131e-02, -2.6553e-01, -1.0066e-01],\n",
       "                      [ 3.5511e-01, -2.3496e-01, -1.0138e-01,  1.6755e-01, -3.3340e-01,\n",
       "                       -3.3512e-01,  3.2877e-01, -7.0164e-02],\n",
       "                      [-1.8500e-01, -1.8730e-01, -1.0319e-01,  3.6101e-01,  2.2619e-01,\n",
       "                        3.5572e-01,  1.0255e-01,  6.8907e-02],\n",
       "                      [ 1.6621e-01, -1.9881e-01,  1.1670e-01,  4.0639e-01,  2.4455e-01,\n",
       "                       -4.6831e-02, -4.9091e-01,  4.3363e-02],\n",
       "                      [ 2.4479e-01,  3.5642e-01,  5.9360e-02,  2.0022e-01, -2.0563e-01,\n",
       "                       -1.4865e-01, -3.1651e-01, -2.2859e-01],\n",
       "                      [-2.6021e-04, -3.0735e-02, -2.2794e-02,  1.9157e-01, -8.2144e-02,\n",
       "                       -2.1280e-01,  1.8715e-02,  2.6499e-02],\n",
       "                      [ 2.3302e-01,  2.7392e-01, -9.6725e-02, -1.1443e-01, -1.4686e-01,\n",
       "                        4.5029e-01,  1.0609e-01, -2.4797e-01],\n",
       "                      [ 3.1332e-01, -2.1864e-01, -1.0183e-01,  2.3246e-01,  1.7224e-01,\n",
       "                        1.3415e-01,  2.1003e-01,  1.0112e-01],\n",
       "                      [ 1.4728e-03,  1.6251e-01, -1.6984e-01, -1.0899e-03,  1.9079e-01,\n",
       "                       -2.2283e-03, -6.2327e-02, -2.9130e-02],\n",
       "                      [ 5.9324e-02,  2.4256e-02, -2.0570e-01,  1.2691e-01,  1.4316e-01,\n",
       "                        5.7391e-02, -2.2012e-02,  2.3722e-01],\n",
       "                      [-1.2116e-03, -2.7831e-01, -1.3748e-01,  2.4257e-01, -1.9193e-01,\n",
       "                        1.1139e-01,  2.5346e-01, -3.4666e-01],\n",
       "                      [-2.5892e-01,  1.8793e-01, -1.4444e-02,  2.2150e-01, -1.5255e-02,\n",
       "                        4.0093e-01, -1.2309e-01, -7.0128e-02],\n",
       "                      [-1.8045e-01,  3.5203e-02,  1.5554e-01,  2.7282e-01,  2.1330e-01,\n",
       "                        5.4371e-02, -3.3573e-01, -3.1214e-01],\n",
       "                      [ 1.3048e-01, -1.5652e-01, -2.6023e-01,  2.2005e-01,  1.8557e-01,\n",
       "                       -2.7272e-01,  2.3887e-01, -1.3961e-01],\n",
       "                      [-1.9051e-01,  4.4310e-03,  4.2108e-02, -1.3832e-01, -1.8984e-01,\n",
       "                        5.5646e-02,  3.3021e-02, -3.4035e-01],\n",
       "                      [-2.1296e-03, -1.6356e-01, -1.1649e-01,  3.1016e-01, -6.8625e-02,\n",
       "                        5.3567e-02, -2.0098e-01, -3.5275e-01],\n",
       "                      [ 2.3639e-01, -2.6126e-01, -1.8624e-01, -3.0165e-01, -1.4550e-01,\n",
       "                        1.3408e-01,  4.9666e-02, -5.9819e-02],\n",
       "                      [ 6.4141e-02,  3.6537e-01,  8.3417e-02,  2.2944e-01,  7.2070e-02,\n",
       "                        2.5428e-01,  2.4747e-01, -1.9553e-02],\n",
       "                      [ 2.0546e-01,  2.6995e-01,  2.0779e-01, -1.3852e-01, -3.4802e-01,\n",
       "                        6.6632e-02, -7.4487e-02, -3.1351e-02],\n",
       "                      [-1.4411e-01,  2.9266e-01, -3.9736e-02,  2.8416e-01,  1.4601e-01,\n",
       "                        8.9521e-02,  4.2496e-01, -1.2788e-01],\n",
       "                      [ 1.1396e-01,  7.2630e-02, -3.3320e-01,  1.2214e-01,  1.5913e-01,\n",
       "                        2.3377e-01, -6.7228e-02,  2.0401e-01],\n",
       "                      [ 7.6237e-02,  1.5686e-02,  5.9340e-02, -3.2755e-01,  2.7031e-01,\n",
       "                       -1.9103e-01, -5.2567e-02,  4.4806e-01],\n",
       "                      [-2.7723e-01, -1.5345e-01, -2.3827e-01, -1.7236e-01,  3.3052e-01,\n",
       "                        1.9993e-01, -3.4964e-01, -2.0513e-01],\n",
       "                      [-3.2626e-01,  3.1189e-02, -1.3394e-01, -2.4441e-01,  9.3233e-02,\n",
       "                        3.1432e-01, -9.7620e-02, -2.6507e-01],\n",
       "                      [-7.6563e-02,  1.5353e-02, -8.6886e-02,  1.4484e-01, -3.1507e-01,\n",
       "                        4.5688e-02, -1.0237e-01,  4.0313e-02],\n",
       "                      [ 8.0702e-02, -2.9622e-01, -1.2952e-01, -6.6647e-02,  5.7610e-02,\n",
       "                       -1.2224e-02, -1.9849e-01, -2.9155e-01],\n",
       "                      [ 4.9993e-02, -1.2743e-01,  2.1653e-01, -3.4629e-01,  3.1166e-01,\n",
       "                       -3.5494e-01,  3.3938e-01, -3.4995e-01],\n",
       "                      [ 3.2621e-01, -6.4833e-02,  2.1743e-01,  5.8120e-02, -3.6504e-02,\n",
       "                        2.3683e-02,  2.3442e-01,  3.7733e-01],\n",
       "                      [-2.7162e-01, -1.5112e-01,  1.8662e-01,  2.5499e-01, -4.2588e-01,\n",
       "                        2.3597e-01,  3.7264e-01,  3.6103e-01],\n",
       "                      [-3.3250e-02, -1.7142e-01, -3.0318e-01, -4.5909e-02, -1.6559e-01,\n",
       "                        4.9974e-02,  3.0032e-01, -1.3424e-01],\n",
       "                      [-3.5410e-01,  2.7729e-01,  1.1189e-02, -2.0269e-01,  1.7827e-01,\n",
       "                        4.4824e-04, -1.1197e-01,  1.6900e-01],\n",
       "                      [-9.7029e-02, -3.6900e-02,  3.0906e-01, -5.1819e-02, -3.7595e-01,\n",
       "                        2.4321e-01, -3.0649e-01, -1.9737e-02],\n",
       "                      [ 1.8441e-01,  1.1235e-01, -1.5027e-02, -8.8796e-02,  2.7206e-01,\n",
       "                       -1.5456e-01,  1.6161e-01,  6.8463e-02],\n",
       "                      [ 2.1057e-01,  2.7762e-01,  3.1807e-01, -3.5723e-01,  4.7763e-02,\n",
       "                       -1.4078e-01, -9.1331e-02,  2.8689e-01],\n",
       "                      [ 2.4573e-01,  5.4836e-02, -1.5219e-01,  1.8900e-01, -1.3487e-01,\n",
       "                       -2.8499e-01, -4.0193e-02,  1.3376e-01],\n",
       "                      [ 2.1948e-01,  1.6532e-01, -1.9309e-02,  1.8888e-01,  3.0691e-02,\n",
       "                       -3.8489e-01,  3.8850e-01, -2.7097e-01],\n",
       "                      [-3.1048e-01, -1.1681e-01, -2.4751e-02, -2.8665e-01,  3.1925e-01,\n",
       "                        4.4721e-01, -3.6278e-01, -9.1410e-02],\n",
       "                      [-1.7686e-01, -2.5962e-01,  3.1208e-01,  3.3534e-01, -1.3192e-01,\n",
       "                        1.6927e-01, -3.4975e-01, -3.8804e-01],\n",
       "                      [ 2.5761e-02,  1.5107e-02, -3.0989e-01,  2.0709e-01,  1.9217e-01,\n",
       "                        1.0353e-01,  2.3572e-01,  1.6574e-02],\n",
       "                      [-5.8064e-02, -2.3953e-02, -4.0225e-02, -3.0566e-01,  2.6384e-01,\n",
       "                        1.9315e-01,  2.2667e-01, -2.9197e-01],\n",
       "                      [ 9.0418e-02, -9.9492e-02, -2.2793e-01,  6.8807e-03, -1.1235e-01,\n",
       "                       -8.2523e-04, -6.6188e-02, -1.1802e-01],\n",
       "                      [-2.6054e-01, -1.9830e-01,  9.2513e-02, -1.0849e-01,  2.7090e-01,\n",
       "                       -3.2156e-01,  1.8791e-02, -8.4362e-02],\n",
       "                      [-8.6774e-02,  2.1032e-01, -9.5709e-02, -2.3624e-01,  2.6630e-01,\n",
       "                       -2.0626e-01, -1.2825e-01, -1.1854e-01],\n",
       "                      [ 3.2107e-01,  6.1530e-03, -2.9564e-01,  2.0937e-01,  7.5738e-02,\n",
       "                        2.8175e-01,  4.0770e-01, -2.7833e-02],\n",
       "                      [-1.4214e-01, -1.4379e-01,  4.0379e-01,  2.9455e-02,  3.5338e-01,\n",
       "                       -1.7906e-01,  3.2127e-01, -1.5294e-01],\n",
       "                      [-1.8870e-01, -2.8895e-01,  2.6556e-01,  2.4724e-01,  1.0149e-01,\n",
       "                       -3.7787e-01, -7.2253e-02,  3.9897e-01],\n",
       "                      [-2.4459e-01,  1.4596e-01,  8.4651e-02, -2.6649e-01,  2.7235e-01,\n",
       "                        1.3848e-01, -3.4695e-01,  4.0581e-01],\n",
       "                      [-2.5351e-01,  2.7215e-01, -1.2894e-01, -8.4026e-02, -7.6757e-02,\n",
       "                        7.6908e-02, -3.3921e-01,  1.1949e-01],\n",
       "                      [-6.0151e-02,  3.5595e-02,  3.6021e-01, -2.2349e-01, -1.0635e-01,\n",
       "                       -4.0421e-01,  9.1414e-02, -3.1305e-01],\n",
       "                      [-2.0231e-01,  1.0379e-01, -2.1422e-01, -1.0457e-01,  1.3949e-01,\n",
       "                        8.0717e-02, -1.7561e-01, -1.9035e-01],\n",
       "                      [ 3.3367e-01, -3.3919e-01, -8.3391e-02, -2.7809e-01, -1.4468e-01,\n",
       "                       -1.9917e-01,  2.7844e-01,  2.8383e-01],\n",
       "                      [ 2.1619e-02, -2.6709e-01, -5.3459e-02, -8.9204e-02,  3.9421e-01,\n",
       "                        4.1202e-01, -2.5421e-01,  1.2915e-01],\n",
       "                      [-7.9737e-02, -2.2407e-01,  1.8022e-01,  3.8313e-01, -3.3645e-01,\n",
       "                        4.6386e-02, -3.7008e-01, -2.1526e-01],\n",
       "                      [ 6.1305e-02,  1.1428e-02, -3.3594e-01, -2.9690e-01,  3.4150e-01,\n",
       "                        1.2349e-02,  2.9381e-01, -2.8088e-01],\n",
       "                      [-1.8830e-01, -1.7342e-01,  3.0323e-01, -7.9755e-02, -2.8158e-01,\n",
       "                        1.9726e-01,  1.6314e-01,  4.2158e-01],\n",
       "                      [ 1.6065e-01, -1.4041e-01, -2.7024e-01, -3.6556e-02, -2.9432e-01,\n",
       "                       -1.1284e-01, -3.5890e-02, -6.0291e-02],\n",
       "                      [-2.8066e-01, -2.7659e-01,  3.6691e-01,  2.0585e-02, -1.4182e-02,\n",
       "                        2.1309e-01,  2.6159e-01, -8.2481e-02],\n",
       "                      [-1.5822e-01, -2.7383e-01, -2.1974e-01, -2.0634e-02, -2.9640e-01,\n",
       "                        7.2374e-02, -6.9304e-02, -5.3406e-02],\n",
       "                      [-3.1601e-01, -1.7693e-01,  1.3900e-01,  3.1822e-01,  3.7610e-01,\n",
       "                       -6.2845e-02, -2.5179e-01, -3.9524e-01],\n",
       "                      [ 5.8909e-02,  5.0751e-02, -1.9535e-01, -3.6650e-02,  3.1098e-01,\n",
       "                       -2.5964e-01,  1.0291e-01,  1.1339e-01]])),\n",
       "             ('input.bias',\n",
       "              tensor([ 2.4731e-01,  2.8819e-01,  1.6291e-01, -2.4029e-01, -1.3163e-01,\n",
       "                       1.5387e-01,  2.8202e-01, -1.2758e-01, -9.1824e-02, -9.9599e-02,\n",
       "                       1.5393e-01,  6.7681e-02, -7.1386e-02, -2.9205e-01,  4.6086e-02,\n",
       "                       5.4357e-02, -2.8803e-01, -3.4925e-01, -2.5890e-01, -2.0306e-01,\n",
       "                      -6.1600e-02, -8.5669e-02, -3.0405e-01, -1.4756e-01,  2.5157e-01,\n",
       "                       3.4171e-02, -8.7757e-02, -2.2939e-01,  1.3497e-01,  1.1466e-01,\n",
       "                      -1.2571e-01,  1.6810e-01,  2.3310e-01,  1.1273e-01,  1.1259e-01,\n",
       "                       2.2456e-01, -5.6399e-02,  2.2074e-01, -1.4812e-02, -9.0626e-02,\n",
       "                       8.7968e-02,  2.4239e-01, -3.0660e-01,  2.9038e-01, -1.4790e-01,\n",
       "                       3.2160e-01, -2.2594e-01, -2.6325e-02, -1.6063e-01, -2.8037e-01,\n",
       "                       7.2257e-02, -1.8176e-01, -8.1915e-02,  1.6712e-01, -3.5140e-01,\n",
       "                       3.1956e-01,  2.9256e-01,  2.8054e-01, -3.1700e-01,  3.4929e-01,\n",
       "                      -1.3475e-01, -1.7583e-04,  1.6561e-02, -2.6938e-01])),\n",
       "             ('hidden1.weight',\n",
       "              tensor([[-0.0323,  0.0114, -0.0011,  ..., -0.0393, -0.0201,  0.0774],\n",
       "                      [ 0.0078, -0.0782, -0.1666,  ..., -0.0625,  0.1212,  0.0980],\n",
       "                      [-0.1139,  0.0510, -0.1165,  ..., -0.0548,  0.0726,  0.0506],\n",
       "                      ...,\n",
       "                      [ 0.0540,  0.1027, -0.1571,  ...,  0.0548,  0.0048, -0.0389],\n",
       "                      [-0.1068,  0.1466,  0.0462,  ..., -0.0857, -0.0156,  0.0309],\n",
       "                      [-0.0745,  0.1354, -0.0944,  ..., -0.0921,  0.0718, -0.0514]])),\n",
       "             ('hidden1.bias',\n",
       "              tensor([ 0.0313,  0.0066, -0.0282,  0.0825, -0.1286, -0.1111,  0.0793,  0.0577,\n",
       "                       0.0687,  0.0897, -0.0676, -0.0026, -0.0579, -0.0632,  0.0471,  0.0255,\n",
       "                       0.0627, -0.0159, -0.0712,  0.0357,  0.1141, -0.0808, -0.0324,  0.0535,\n",
       "                       0.0265,  0.0352, -0.0453, -0.0690,  0.0051, -0.0384, -0.0199, -0.0615,\n",
       "                       0.0706, -0.0952,  0.1052,  0.0737, -0.0185,  0.1137,  0.0429,  0.1163,\n",
       "                      -0.0659, -0.1153,  0.0631,  0.0486,  0.1096, -0.0262, -0.0149, -0.0969,\n",
       "                      -0.1008, -0.0382,  0.1262,  0.0008,  0.0149,  0.1106,  0.0910,  0.0657,\n",
       "                      -0.0671,  0.0615,  0.0290,  0.0785,  0.0437,  0.0306, -0.0179,  0.0982])),\n",
       "             ('hidden2.weight',\n",
       "              tensor([[ 0.0389, -0.0311, -0.0209,  ...,  0.1325, -0.0931,  0.0919],\n",
       "                      [-0.1768,  0.2152,  0.0430,  ...,  0.0673, -0.1338,  0.0258],\n",
       "                      [ 0.0811,  0.0438, -0.0963,  ..., -0.1104,  0.0545, -0.0872],\n",
       "                      ...,\n",
       "                      [ 0.0854, -0.0242, -0.1043,  ...,  0.0144, -0.0338, -0.0621],\n",
       "                      [-0.0710, -0.0794,  0.1771,  ...,  0.1564,  0.0195,  0.1185],\n",
       "                      [-0.1910,  0.0037, -0.0573,  ..., -0.0580,  0.0721, -0.0158]])),\n",
       "             ('hidden2.bias',\n",
       "              tensor([-0.0298,  0.0111,  0.0451, -0.1077,  0.1264,  0.0986, -0.1093, -0.1008,\n",
       "                      -0.0043, -0.0297,  0.0140, -0.0175,  0.0032,  0.0391,  0.1063,  0.0819,\n",
       "                       0.1142, -0.0918, -0.0693,  0.1123,  0.1225, -0.1124, -0.0662,  0.0115,\n",
       "                      -0.0548,  0.0285,  0.0010,  0.0971,  0.0017]))])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('input.weight',\n",
       "              tensor([[ 1.4240e-01,  3.0208e-01,  3.3952e-01,  4.0957e-01, -2.1840e-01,\n",
       "                        7.7345e-02, -1.1007e-01, -6.7018e-02],\n",
       "                      [-1.6411e-01, -3.2685e-01, -3.2188e-02, -1.7808e-01,  2.6823e-01,\n",
       "                       -2.3882e-01,  1.7069e-01,  2.4261e-01],\n",
       "                      [-2.9862e-01,  1.3380e-01,  3.0032e-01,  8.4978e-02, -1.1280e-01,\n",
       "                        3.9817e-01,  3.1366e-01, -8.1339e-02],\n",
       "                      [-3.3819e-01, -7.2151e-02,  2.0203e-01, -3.1742e-01,  1.6309e-01,\n",
       "                       -3.4058e-01,  2.2519e-01,  2.2271e-01],\n",
       "                      [-1.3653e-01,  3.2881e-01, -2.8039e-01, -7.7726e-02, -1.7620e-01,\n",
       "                        7.9638e-02,  2.9207e-01, -2.4837e-01],\n",
       "                      [-2.6533e-01, -2.8714e-01,  2.9645e-02,  2.8596e-01,  2.1628e-01,\n",
       "                        2.1521e-01,  2.7535e-01, -3.3423e-01],\n",
       "                      [ 2.9354e-01, -1.7299e-01, -2.2842e-01,  1.5060e-01, -2.1313e-01,\n",
       "                       -1.2420e-01,  2.3784e-01,  1.7455e-02],\n",
       "                      [ 2.6589e-03, -2.3575e-01, -2.9266e-01,  1.5451e-01, -2.9297e-01,\n",
       "                        9.6555e-03,  4.6425e-02,  1.2983e-01],\n",
       "                      [ 7.8113e-02, -3.5014e-01,  2.0354e-01,  8.9727e-02,  3.4550e-01,\n",
       "                        5.4940e-02, -3.2907e-01,  3.1784e-02],\n",
       "                      [ 8.5027e-02,  3.2266e-01, -2.1427e-01, -1.4585e-01, -1.3510e-01,\n",
       "                        2.3001e-01,  4.6539e-02, -2.3621e-01],\n",
       "                      [ 2.4505e-01,  1.2278e-01, -2.4933e-01,  3.8219e-01,  4.5098e-01,\n",
       "                       -5.6703e-02, -2.0738e-02, -2.7720e-01],\n",
       "                      [ 2.8449e-01, -3.4173e-01,  1.7214e-01,  2.4358e-01, -3.7912e-01,\n",
       "                       -5.6290e-02, -1.8460e-01,  1.6423e-01],\n",
       "                      [-8.0803e-02,  2.2736e-01, -3.2282e-01,  3.0734e-01, -8.4570e-02,\n",
       "                        2.3535e-01, -2.9111e-01,  2.4128e-02],\n",
       "                      [-3.5145e-01,  2.9469e-01, -1.0473e-02, -1.2858e-01, -1.7325e-01,\n",
       "                        2.3049e-01,  3.1718e-01, -6.4269e-02],\n",
       "                      [ 3.1155e-01,  1.8785e-01, -2.7525e-01, -3.2683e-01,  1.3108e-01,\n",
       "                       -4.5039e-02,  7.1295e-02,  1.0590e-01],\n",
       "                      [-1.4149e-02, -5.6659e-02,  2.2329e-01, -5.4066e-02, -1.3932e-01,\n",
       "                        1.6134e-01, -1.8559e-01, -2.7663e-01],\n",
       "                      [ 1.2911e-01,  5.1494e-02,  8.2068e-02, -1.2603e-01, -6.4939e-02,\n",
       "                        2.7277e-01,  4.2378e-02, -1.8285e-01],\n",
       "                      [-3.1099e-01,  1.3142e-01,  2.0098e-01, -1.3360e-01,  2.3552e-02,\n",
       "                       -3.0665e-01,  4.4835e-02,  3.0397e-01],\n",
       "                      [ 2.8312e-01, -3.4624e-01,  4.7223e-02, -3.3249e-01, -2.3504e-01,\n",
       "                        2.7385e-01,  3.2831e-01,  2.2339e-01],\n",
       "                      [-1.9647e-01, -3.1262e-01,  3.7448e-01, -2.3283e-01,  2.9190e-01,\n",
       "                        3.9999e-01,  1.4839e-01,  2.5432e-02],\n",
       "                      [ 3.2453e-01, -1.7407e-01, -2.4976e-01,  1.9212e-01,  3.1824e-01,\n",
       "                        2.0594e-01, -1.4348e-01,  3.0517e-01],\n",
       "                      [ 1.7710e-01, -2.3167e-01, -6.3289e-02,  2.4409e-01,  2.6072e-02,\n",
       "                        3.2994e-01, -2.2519e-01,  3.3582e-01],\n",
       "                      [ 2.0413e-01,  4.8716e-02,  1.2877e-01,  1.7005e-01, -1.1270e-01,\n",
       "                       -3.4320e-01,  9.2843e-02,  2.6733e-01],\n",
       "                      [ 1.3660e-01, -8.6473e-02,  1.3970e-01, -1.1355e-01, -1.2986e-01,\n",
       "                       -2.2345e-01,  2.1543e-01, -2.2261e-01],\n",
       "                      [-1.3919e-01, -1.2147e-01,  3.6175e-01, -7.0732e-03,  3.3509e-01,\n",
       "                        1.5589e-01,  2.1116e-02, -1.4959e-01],\n",
       "                      [ 2.8718e-01, -1.2129e-01,  2.2707e-01,  2.3529e-02,  3.3529e-01,\n",
       "                       -3.6892e-01, -2.8835e-01, -5.6738e-03],\n",
       "                      [-1.0630e-01,  3.3397e-01, -3.0560e-01,  3.3230e-01,  2.5918e-01,\n",
       "                        1.7609e-01,  3.5533e-01,  1.3317e-01],\n",
       "                      [-5.3544e-02,  1.8228e-01,  1.7682e-01,  3.3052e-01,  6.7296e-02,\n",
       "                        2.0060e-01, -4.6851e-01, -3.7490e-01],\n",
       "                      [ 7.3580e-02, -1.5881e-01,  3.7242e-01,  3.9363e-01, -3.1687e-01,\n",
       "                       -1.8624e-01,  2.7052e-01, -2.8958e-01],\n",
       "                      [ 2.1871e-01,  2.6717e-01,  1.0754e-01,  2.2469e-01, -7.2467e-02,\n",
       "                       -5.8409e-02,  2.6826e-01,  2.8667e-01],\n",
       "                      [-2.4402e-01, -2.9252e-01, -6.8823e-02,  1.5803e-01,  3.6501e-01,\n",
       "                        1.6458e-01,  3.8882e-01, -6.6492e-02],\n",
       "                      [-1.3059e-02,  3.1041e-02, -2.3342e-01, -2.2651e-01,  1.9279e-01,\n",
       "                        2.1095e-01, -8.1233e-02,  1.1332e-01],\n",
       "                      [ 3.0436e-01,  1.1376e-01, -3.1887e-01, -3.5339e-01,  2.1493e-01,\n",
       "                       -1.2574e-01, -2.7642e-01,  2.3652e-01],\n",
       "                      [-2.4928e-01,  1.7518e-01,  1.2429e-01,  2.1034e-01, -1.9285e-01,\n",
       "                        2.2077e-01, -3.2459e-01,  3.2868e-01],\n",
       "                      [-3.2679e-01, -1.7558e-01,  6.1477e-02, -2.2173e-01,  1.3136e-01,\n",
       "                        2.5048e-01, -3.0098e-01, -6.5929e-02],\n",
       "                      [-2.6523e-01,  9.6640e-02, -3.3599e-01, -1.4578e-01,  4.5193e-02,\n",
       "                        1.8626e-01, -3.4052e-01, -3.3796e-01],\n",
       "                      [ 1.6688e-01,  3.1369e-01, -2.7203e-01, -3.3376e-01, -1.1129e-02,\n",
       "                        2.7787e-01, -1.7464e-01,  2.5820e-01],\n",
       "                      [-2.9010e-01,  1.8629e-01,  2.5278e-01,  2.3111e-01, -2.6883e-01,\n",
       "                       -1.4575e-01, -1.9943e-01, -3.1558e-01],\n",
       "                      [-2.7100e-01, -4.9177e-02,  1.0142e-01,  5.4107e-02,  4.5620e-02,\n",
       "                        1.5108e-01,  1.6490e-01,  7.9315e-02],\n",
       "                      [ 2.2190e-01, -3.2768e-02,  2.3176e-02,  1.8896e-01, -3.8715e-01,\n",
       "                        5.4354e-02,  3.5764e-01, -1.5883e-01],\n",
       "                      [ 1.8485e-01,  2.4250e-01,  1.4479e-01, -5.2998e-02,  3.4914e-03,\n",
       "                       -1.2099e-01, -1.3709e-01, -4.2240e-01],\n",
       "                      [ 3.8283e-02,  2.6796e-01,  6.2880e-02,  3.7564e-02,  8.6948e-03,\n",
       "                       -2.7683e-01,  1.4914e-02,  2.3113e-01],\n",
       "                      [ 7.1803e-02,  1.7245e-01, -6.5713e-02,  2.0823e-01,  1.5971e-02,\n",
       "                       -2.5547e-01, -1.3505e-01, -1.9164e-01],\n",
       "                      [ 2.5191e-01, -2.7316e-02,  3.0647e-01,  1.2887e-01, -3.8865e-01,\n",
       "                       -9.8361e-02, -3.9169e-03, -2.8961e-02],\n",
       "                      [-2.5474e-01, -1.2103e-02,  6.3302e-02, -3.4051e-01, -2.2493e-01,\n",
       "                       -2.8006e-01, -2.7011e-01,  1.7227e-01],\n",
       "                      [-1.8457e-01,  1.5206e-01, -1.2869e-01,  2.0489e-02, -2.6576e-01,\n",
       "                        9.1884e-02, -2.3830e-01, -2.9322e-01],\n",
       "                      [-2.0632e-01, -1.8689e-01,  2.6278e-01, -3.2214e-01,  8.6991e-02,\n",
       "                       -3.7381e-01,  3.2853e-01, -4.7730e-02],\n",
       "                      [ 2.7897e-01,  2.3755e-01, -1.7298e-03, -3.5956e-02, -4.0767e-01,\n",
       "                        2.3393e-01,  1.7414e-01,  3.2188e-01],\n",
       "                      [-9.0364e-02, -2.4558e-01,  3.5426e-02,  1.9119e-01,  2.8196e-01,\n",
       "                       -3.8903e-01,  3.1892e-01, -1.1739e-01],\n",
       "                      [ 1.9904e-01, -3.0322e-01, -2.3624e-01,  2.9491e-01,  4.4340e-02,\n",
       "                        1.3812e-01, -3.0831e-02,  2.3630e-01],\n",
       "                      [-7.2031e-02, -3.2603e-01, -1.5851e-01,  2.6882e-01,  2.0769e-02,\n",
       "                       -6.8604e-02, -8.8316e-02,  8.9982e-04],\n",
       "                      [ 1.8432e-01,  1.0798e-01, -9.4132e-02,  1.6111e-01,  1.5177e-01,\n",
       "                        3.7928e-01, -1.2328e-01, -3.9921e-01],\n",
       "                      [ 1.7192e-01, -1.1539e-01, -3.0341e-01,  2.0377e-01,  1.8632e-01,\n",
       "                        1.7588e-02, -3.2047e-01,  1.2105e-01],\n",
       "                      [-2.3737e-01,  1.7700e-01,  1.0430e-01,  2.0157e-01, -2.4299e-01,\n",
       "                       -2.1813e-01,  2.6657e-01,  1.6509e-01],\n",
       "                      [-1.3859e-01,  2.1819e-01,  3.1393e-01,  2.7897e-02, -2.3338e-01,\n",
       "                       -2.5321e-01, -2.2650e-02, -3.3400e-01],\n",
       "                      [ 3.0648e-01, -3.0164e-01, -6.2389e-02,  1.3987e-01, -2.8120e-01,\n",
       "                        4.0974e-01,  3.0965e-01,  3.3536e-01],\n",
       "                      [-2.1000e-01,  3.2076e-01,  2.3667e-01,  2.1717e-02, -3.6301e-02,\n",
       "                       -2.6302e-01,  3.4149e-01, -1.7640e-01],\n",
       "                      [ 3.2111e-02, -2.4993e-01, -4.9189e-02,  3.3237e-01,  7.8668e-02,\n",
       "                       -2.6746e-01, -4.0974e-02,  9.1779e-02],\n",
       "                      [ 1.5217e-01, -3.3670e-01, -8.4497e-02,  3.4645e-01, -2.1971e-01,\n",
       "                       -2.2276e-01,  2.2474e-01,  2.9539e-01],\n",
       "                      [-3.7468e-02,  1.2798e-01,  1.0940e-01, -6.8459e-02, -2.7274e-01,\n",
       "                       -2.6102e-01, -2.9084e-01,  1.2440e-01],\n",
       "                      [ 2.7379e-01, -1.1980e-01,  2.4839e-01, -3.0727e-01,  4.5219e-01,\n",
       "                        1.6663e-01, -7.2373e-02,  6.9790e-02],\n",
       "                      [-2.7292e-01, -9.2614e-02,  2.4592e-01, -1.2056e-01,  1.3145e-01,\n",
       "                       -8.8693e-02, -2.7771e-01, -3.0931e-01],\n",
       "                      [-1.9597e-01,  1.5920e-01,  8.4127e-02,  3.0504e-01,  1.7131e-01,\n",
       "                       -2.2892e-01,  3.5955e-01, -3.1694e-01],\n",
       "                      [ 1.8318e-01,  2.6741e-01, -8.1156e-02,  4.0497e-04,  1.2991e-01,\n",
       "                       -7.3469e-02, -3.2383e-02,  3.9939e-01]])),\n",
       "             ('input.bias',\n",
       "              tensor([ 0.2941, -0.0799,  0.1280, -0.1956, -0.1107,  0.3220,  0.1479,  0.1670,\n",
       "                      -0.2371, -0.2083, -0.1401,  0.0170,  0.1307,  0.1765,  0.1371, -0.3100,\n",
       "                      -0.0076, -0.2627,  0.3165,  0.1773,  0.0164,  0.0106,  0.1997, -0.1731,\n",
       "                       0.0285,  0.2929,  0.0253, -0.3077, -0.1535,  0.0208, -0.2103, -0.0501,\n",
       "                      -0.0378, -0.2279, -0.0216,  0.3034, -0.3316,  0.1552, -0.0743,  0.1736,\n",
       "                       0.0817, -0.2727,  0.2772, -0.3124,  0.2021, -0.3382, -0.2015,  0.2622,\n",
       "                      -0.0081,  0.0797,  0.1528,  0.2474, -0.2514, -0.1594,  0.2204, -0.2160,\n",
       "                       0.1267, -0.2843, -0.1900,  0.0559, -0.3175, -0.1287,  0.3001,  0.2452])),\n",
       "             ('hidden1.weight',\n",
       "              tensor([[ 0.1317,  0.1041,  0.0507,  ...,  0.0642,  0.1762, -0.0729],\n",
       "                      [ 0.0115,  0.1022, -0.0746,  ..., -0.0462, -0.0398, -0.0670],\n",
       "                      [ 0.0086,  0.0023, -0.0725,  ..., -0.0598, -0.0778,  0.1119],\n",
       "                      ...,\n",
       "                      [ 0.0718, -0.0836, -0.0881,  ...,  0.1239, -0.0205, -0.0419],\n",
       "                      [ 0.0353, -0.0869, -0.1308,  ..., -0.0697,  0.0790, -0.0519],\n",
       "                      [-0.1535,  0.0661, -0.1263,  ...,  0.1270,  0.0762, -0.0231]])),\n",
       "             ('hidden1.bias',\n",
       "              tensor([-0.0837,  0.0528,  0.0471, -0.0330, -0.0937,  0.0228,  0.0784,  0.0476,\n",
       "                       0.0753, -0.0720,  0.1030, -0.1250, -0.0274,  0.0541,  0.0077,  0.1243,\n",
       "                       0.1107,  0.1078, -0.0186,  0.0836,  0.0611,  0.1095,  0.0335, -0.1057,\n",
       "                      -0.0592,  0.1089, -0.1088,  0.0305,  0.1072,  0.0276,  0.0150, -0.0728,\n",
       "                       0.1230,  0.1081, -0.0007, -0.0325, -0.0560, -0.0682,  0.1041, -0.1077,\n",
       "                      -0.0477,  0.0168, -0.1245,  0.0220, -0.1180,  0.1093,  0.0417, -0.0987,\n",
       "                      -0.0869, -0.1038, -0.0975, -0.1265, -0.0021,  0.0685,  0.0409,  0.0333,\n",
       "                       0.0489, -0.0657, -0.0154,  0.0537, -0.1195, -0.1181, -0.0955, -0.1165])),\n",
       "             ('hidden2.weight',\n",
       "              tensor([[-0.0340,  0.1065,  0.0687,  ...,  0.0448,  0.1381, -0.0038],\n",
       "                      [ 0.1607,  0.1102,  0.1083,  ...,  0.0186, -0.0368,  0.0495],\n",
       "                      [-0.0215, -0.0338, -0.1230,  ...,  0.1032, -0.1062, -0.0847],\n",
       "                      ...,\n",
       "                      [-0.1179,  0.0893, -0.0299,  ..., -0.0110,  0.0177, -0.0288],\n",
       "                      [ 0.2045,  0.0459,  0.0515,  ...,  0.0153,  0.1197, -0.0975],\n",
       "                      [-0.0330, -0.1127,  0.0100,  ..., -0.0373,  0.0511,  0.0419]])),\n",
       "             ('hidden2.bias',\n",
       "              tensor([-0.0032, -0.0048, -0.0440,  0.0457, -0.1178, -0.0296,  0.0782,  0.0486,\n",
       "                      -0.0228,  0.0174,  0.1298,  0.1050,  0.0650,  0.1036,  0.0774, -0.0513,\n",
       "                      -0.0483, -0.0956,  0.1031, -0.0412,  0.0947,  0.0521, -0.0196,  0.0360,\n",
       "                       0.0811,  0.0158,  0.0629,  0.0983, -0.0589]))])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model2 = Model()\n",
    "model2.load_state_dict(torch.load('crop_prediction_weights.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.1137e-03, 1.0728e-04, 4.4431e-06, 9.9732e-07, 3.2659e-02, 1.3428e-02,\n         3.3147e-01, 1.3052e-03, 2.6530e-04, 3.1458e-04, 9.3813e-04, 2.3722e-02,\n         2.4871e-01, 1.7424e-02, 5.0516e-04, 1.7932e-02, 1.7657e-03, 5.1582e-07,\n         1.3871e-01, 5.3930e-02, 1.2608e-05, 7.1804e-07, 4.4971e-03, 7.4200e-02,\n         5.7790e-04, 3.1376e-03, 1.7156e-06, 3.3251e-02, 1.7217e-05]],\n       grad_fn=<SoftmaxBackward>)\nCrop:- arhar  Probab:- 0.11136536300182343\nCrop:- bajra  Probab:- 0.010728138498961926\nCrop:- barley  Probab:- 0.00044431290007196367\nCrop:- coriander  Probab:- 9.973191481549293e-05\nCrop:- cotton  Probab:- 3.265881061553955\nCrop:- cowpea  Probab:- 1.342787742614746\nCrop:- dry chillies  Probab:- 33.14677047729492\nCrop:- garlic  Probab:- 0.13052260875701904\nCrop:- ginger  Probab:- 0.0265301875770092\nCrop:- gram  Probab:- 0.031458210200071335\nCrop:- groundnut  Probab:- 0.09381312131881714\nCrop:- jowar  Probab:- 2.3722035884857178\nCrop:- linseed  Probab:- 24.871213912963867\nCrop:- maize-k  Probab:- 1.742421269416809\nCrop:- maize-r  Probab:- 0.050515513867139816\nCrop:- masoor  Probab:- 1.7932209968566895\nCrop:- moong  Probab:- 0.17656825482845306\nCrop:- onion  Probab:- 5.15821848239284e-05\nCrop:- peas&beans  Probab:- 13.870890617370605\nCrop:- potato  Probab:- 5.393012046813965\nCrop:- ragi  Probab:- 0.0012608444085344672\nCrop:- rapeseed  Probab:- 7.180419925134629e-05\nCrop:- rice  Probab:- 0.4497118294239044\nCrop:- safflower  Probab:- 7.419954776763916\nCrop:- sugarcane  Probab:- 0.05778980627655983\nCrop:- sunflower  Probab:- 0.3137584924697876\nCrop:- turmeric  Probab:- 0.00017156045942101628\nCrop:- urad  Probab:- 3.3250536918640137\nCrop:- wheat  Probab:- 0.001721650012768805\n"
     ]
    }
   ],
   "source": [
    "crops = ['arhar', 'bajra', 'barley', 'coriander', 'cotton', 'cowpea', 'dry chillies', 'garlic', 'ginger', 'gram', 'groundnut', 'jowar', 'linseed', 'maize-k', 'maize-r', 'masoor', 'moong', 'onion', 'peas&beans', 'potato', 'ragi', 'rapeseed', 'rice', 'safflower', 'sugarcane', 'sunflower', 'turmeric', 'urad', 'wheat']\n",
    "\n",
    "\n",
    "pred_user = model2(torch.from_numpy(np.array([[20, 7.4, 78, 23, 28, 15, 18, 5]], dtype='float32')))\n",
    "print(pred_user)\n",
    "\n",
    "greater_than_zero={}\n",
    "index = -1\n",
    "\n",
    "for i in pred_user:\n",
    "    for p in i:\n",
    "        index+=1\n",
    "        if(p > 0):\n",
    "            greater_than_zero[crops[index]] = p \n",
    "    for i in greater_than_zero:\n",
    "        print(\"Crop:- {0}  Probab:- {1}\".format(i,greater_than_zero[i]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}