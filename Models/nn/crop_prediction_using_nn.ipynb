{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "crop_prediction_using_linear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGCgzfwTAtCc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoXRW0b_A5uS",
        "outputId": "775183f5-3a17-491f-d54f-68bcf6b65e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crop_prediction_weights_final_BELOW7LOSS_1_LOSS10.pth\n",
            "crop_prediction_weights_final_BELOW7LOSS_1_LOSS9.pth\n",
            "crop_prediction_weights_final_BELOW7LOSS_2_LOSS10.pth\n",
            "crop_prediction_weights_final_BELOW7LOSS_3_LOSS10.pth\n",
            "crop_prediction_weights_final_BELOW7LOSS_4_LOSS10.pth\n",
            "crop_prediction_weights_final_updated_with_harvest.pth\n",
            "crop_prediction_weights_final_updated_without_harvest.pth\n",
            "crops_final_updated.csv\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UA-jtWSAtCn"
      },
      "source": [
        "# reading csv file\n",
        "df = pd.read_csv('crops_final_updated.csv')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8-nhv8sAtCw"
      },
      "source": [
        "# df.drop('Harvest Temp', axis=1,inplace=True)\n",
        "# converting from pandas dataframe to numpy\n",
        "np_inputs = df.to_numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4amgKMRAtC2",
        "outputId": "a33e6007-9c2d-467f-b9b8-00224b581d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# inputs are from col-6 to col-13\n",
        "inputs = np_inputs[:, 6:14]\n",
        "inputs = np.array(inputs, dtype='float32')\n",
        "\n",
        "print(inputs)\n",
        "\n",
        "\n",
        "\n",
        "# convert output crops to binary encoded labels\n",
        "from sklearn import preprocessing\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit(np_inputs[:,14])\n",
        "outputs = lb.transform(np_inputs[:,14])\n",
        "\n",
        "outputs = np.array(outputs, dtype='float32')\n",
        "\n",
        "print(outputs)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[33.259373   7.030931  60.84086   ...  6.714771  17.60041   12.486836 ]\n",
            " [33.324234   6.562556  64.75895   ...  5.5676103 16.45551   11.969066 ]\n",
            " [33.736282   6.557421  61.573425  ...  5.7565145 17.515505  12.502116 ]\n",
            " ...\n",
            " [25.8816     6.181125  30.692217  ... 46.018555  28.015871  11.661039 ]\n",
            " [24.774702   6.929148  58.6065    ... 54.853306  24.418463  12.532048 ]\n",
            " [23.613468   6.6437187 53.63108   ... 48.630276  28.834482  11.292699 ]]\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPwlLHEaAtC-",
        "outputId": "67a3fb48-337f-4609-e4ab-b697aa62479b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# converting numpy array into torch tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "outputs = torch.from_numpy(outputs)\n",
        "\n",
        "print(inputs)\n",
        "print(outputs)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[33.2594,  7.0309, 60.8409,  ...,  6.7148, 17.6004, 12.4868],\n",
            "        [33.3242,  6.5626, 64.7589,  ...,  5.5676, 16.4555, 11.9691],\n",
            "        [33.7363,  6.5574, 61.5734,  ...,  5.7565, 17.5155, 12.5021],\n",
            "        ...,\n",
            "        [25.8816,  6.1811, 30.6922,  ..., 46.0186, 28.0159, 11.6610],\n",
            "        [24.7747,  6.9291, 58.6065,  ..., 54.8533, 24.4185, 12.5320],\n",
            "        [23.6135,  6.6437, 53.6311,  ..., 48.6303, 28.8345, 11.2927]])\n",
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMM_qQdmAtDD"
      },
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7cdy2wjAtDI",
        "outputId": "8a802645-ba40-4534-9ce1-59cb01112cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "ds = TensorDataset(inputs,outputs)\n",
        "print(len(ds))\n",
        "print(ds[0:3])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121081\n",
            "(tensor([[33.2594,  7.0309, 60.8409, 32.7850, 37.2703,  6.7148, 17.6004, 12.4868],\n",
            "        [33.3242,  6.5626, 64.7589, 29.3024, 36.9567,  5.5676, 16.4555, 11.9691],\n",
            "        [33.7363,  6.5574, 61.5734, 29.1465, 36.2751,  5.7565, 17.5155, 12.5021]]), tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmOxbJlDAtDN"
      },
      "source": [
        "from torch.utils.data import random_split"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l6JrgcJAtDR",
        "outputId": "f9b2cebe-0af4-4339-eef0-2b5a01d03f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_ds, valid_ds = random_split(ds, [97065, 24016])\n",
        "len(train_ds), len(valid_ds)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97065, 24016)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5xzh8YAtDW"
      },
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVvPmSIYAtDa",
        "outputId": "75039a5b-7d3d-4cf9-dfb7-4af536a4e629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds , batch_size , shuffle=True )\n",
        "val_loader = DataLoader(valid_ds , batch_size)\n",
        "print(len(train_loader))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USH3W_XcAtDf"
      },
      "source": [
        "# Initial network\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "# defining neural network parameteres\n",
        "input_nodes = 8\n",
        "hidden1_nodes = 64\n",
        "hidden2_nodes = 64\n",
        "output_nodes = 29\n",
        "\n",
        "# creating neural net\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input = nn.Linear(input_nodes, hidden1_nodes)\n",
        "        self.hidden1 = nn.Linear(hidden1_nodes, hidden2_nodes)\n",
        "        self.hidden2 = nn.Linear(hidden2_nodes, output_nodes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeY5uXYFzGQG"
      },
      "source": [
        "# Improved model\n",
        "from torch import nn\n",
        "\n",
        "# defining neural network parameteres\n",
        "input_nodes = 8\n",
        "hidden1_nodes = 32\n",
        "hidden2_nodes = 64\n",
        "output_nodes = 29\n",
        "\n",
        "# creating neural net\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input = nn.Linear(input_nodes, hidden1_nodes)\n",
        "        self.hidden1 = nn.Linear(hidden1_nodes, hidden2_nodes)\n",
        "        self.hidden2 = nn.Linear(hidden2_nodes, output_nodes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDleErFAtDj",
        "outputId": "b3f17845-40f8-497d-864a-4cf297973c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model()\n",
        "list(model.parameters())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.3362,  0.1892, -0.0362, -0.2680, -0.2745,  0.0404,  0.0576,  0.2466],\n",
              "         [ 0.0602,  0.2791, -0.3340, -0.2179,  0.2105,  0.2288,  0.3222, -0.2259],\n",
              "         [ 0.2677, -0.3293,  0.0688,  0.0131,  0.3216,  0.1370, -0.2620,  0.0419],\n",
              "         [ 0.2423, -0.2243, -0.0386,  0.2063, -0.0974, -0.0614, -0.1296, -0.2302],\n",
              "         [-0.3189,  0.0197,  0.1058,  0.2875,  0.2469,  0.2285,  0.2189,  0.2858],\n",
              "         [-0.1342, -0.1397,  0.3262, -0.2251, -0.2996, -0.1949, -0.3357, -0.2515],\n",
              "         [ 0.2686,  0.1826,  0.0065,  0.0970, -0.2019,  0.0346,  0.3186, -0.2426],\n",
              "         [ 0.1907, -0.0162,  0.0058, -0.0231, -0.2883, -0.0731, -0.1720, -0.1836],\n",
              "         [ 0.2753,  0.0475,  0.3305,  0.0314,  0.2715,  0.1408,  0.0970,  0.3489],\n",
              "         [-0.0342, -0.3048, -0.1325, -0.1461,  0.0543, -0.1569,  0.0463,  0.1447],\n",
              "         [-0.2079, -0.2469, -0.0304, -0.1956, -0.2297,  0.3450, -0.1952,  0.0017],\n",
              "         [ 0.0446, -0.2094, -0.0657, -0.0116, -0.3317, -0.2069, -0.0514,  0.0663],\n",
              "         [-0.3112,  0.0107, -0.1163,  0.1190, -0.0511,  0.1371, -0.2536, -0.0666],\n",
              "         [-0.1795, -0.0616,  0.2653, -0.2752,  0.1832,  0.1668, -0.2086, -0.0056],\n",
              "         [-0.3379, -0.0336,  0.0033,  0.0316, -0.2120,  0.2800, -0.0698,  0.2395],\n",
              "         [ 0.2436, -0.0511,  0.2766, -0.2420,  0.0698,  0.2251, -0.3044, -0.2511],\n",
              "         [ 0.0247, -0.3472,  0.0168, -0.3248,  0.1355,  0.3290, -0.1045, -0.0860],\n",
              "         [ 0.1064, -0.2767, -0.2523, -0.2168, -0.2096, -0.3482, -0.2864,  0.1574],\n",
              "         [-0.2760,  0.0428, -0.0980,  0.0124, -0.0293,  0.2773,  0.1530, -0.0096],\n",
              "         [ 0.0385,  0.2472, -0.1351, -0.2764, -0.3485,  0.3445, -0.1236,  0.1629],\n",
              "         [-0.1014, -0.0413, -0.1663,  0.0715, -0.3226, -0.1905, -0.2089,  0.0261],\n",
              "         [ 0.3386, -0.0518,  0.1403,  0.1476, -0.3465, -0.0987,  0.0892, -0.3531],\n",
              "         [ 0.0398,  0.2238,  0.2895,  0.1146, -0.1838, -0.0289, -0.2702,  0.2642],\n",
              "         [-0.1111,  0.1472,  0.3348, -0.1783,  0.0693, -0.2691, -0.1924, -0.0972],\n",
              "         [-0.1733, -0.1562,  0.3008, -0.0279, -0.2875, -0.0203,  0.0056, -0.2591],\n",
              "         [-0.2681, -0.3172,  0.1386, -0.1389, -0.3060, -0.0416,  0.1585, -0.1489],\n",
              "         [ 0.1328, -0.2184,  0.1026, -0.1867, -0.0331,  0.1024,  0.3212,  0.0299],\n",
              "         [ 0.0571,  0.1876, -0.1135, -0.2298, -0.1852,  0.2219,  0.1202,  0.0272],\n",
              "         [-0.3087, -0.1245,  0.0274, -0.1961, -0.2387,  0.3450, -0.3204, -0.0878],\n",
              "         [-0.0795,  0.1716, -0.0292, -0.1114, -0.3499, -0.1532, -0.3376,  0.2099],\n",
              "         [ 0.1522,  0.2014,  0.1854, -0.2968, -0.0005,  0.2011, -0.1278,  0.0894],\n",
              "         [-0.1653,  0.2137,  0.2223, -0.3295, -0.2248, -0.1116,  0.0472, -0.0955]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.2548, -0.1087, -0.1893,  0.2409, -0.0312, -0.1406,  0.0054, -0.1589,\n",
              "          0.0271,  0.0372, -0.1682, -0.3001,  0.0124,  0.1072, -0.0448,  0.3361,\n",
              "          0.0827,  0.3051, -0.3319, -0.1339,  0.1171, -0.1641, -0.2121,  0.3360,\n",
              "          0.0522, -0.0803,  0.2895, -0.1957, -0.2269,  0.1933, -0.1196,  0.1484],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.1350, -0.0064, -0.1725,  ..., -0.1409, -0.1377, -0.1408],\n",
              "         [-0.1669, -0.1188, -0.0284,  ...,  0.1472, -0.0902,  0.1697],\n",
              "         [-0.0691,  0.0189, -0.0376,  ..., -0.0500,  0.1270, -0.1332],\n",
              "         ...,\n",
              "         [-0.1026,  0.1398, -0.1517,  ...,  0.0972,  0.1759,  0.0906],\n",
              "         [ 0.1020,  0.0541,  0.1327,  ..., -0.0838, -0.0672,  0.0146],\n",
              "         [ 0.1514, -0.0485, -0.1287,  ..., -0.0354,  0.1072, -0.1247]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.1067,  0.0715,  0.0949,  0.0783, -0.0237,  0.1157,  0.1085, -0.0134,\n",
              "          0.1560, -0.0102, -0.1124, -0.0597, -0.0177, -0.0993, -0.1055,  0.1014,\n",
              "          0.0246, -0.0261, -0.1528,  0.0862,  0.0433,  0.0032,  0.0854, -0.1577,\n",
              "         -0.0724,  0.0586,  0.1298, -0.1454, -0.0576,  0.1656, -0.0081, -0.1191,\n",
              "          0.0919, -0.0352, -0.0716,  0.1592, -0.1552,  0.1477,  0.0723, -0.0138,\n",
              "         -0.0216,  0.0090,  0.1653,  0.0263, -0.0189,  0.0654,  0.0849,  0.1655,\n",
              "         -0.0068, -0.1033, -0.0778,  0.0751, -0.1070, -0.1493,  0.1300,  0.1530,\n",
              "         -0.1494,  0.1728, -0.1653,  0.0458, -0.1073, -0.0203, -0.1590, -0.0166],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.0513, -0.0656,  0.0198,  ..., -0.0581,  0.0792,  0.0188],\n",
              "         [-0.0960,  0.1072,  0.0326,  ..., -0.0894,  0.0725, -0.0401],\n",
              "         [-0.0761,  0.0940, -0.0938,  ...,  0.1211,  0.0365,  0.0070],\n",
              "         ...,\n",
              "         [ 0.1061,  0.0918,  0.0991,  ..., -0.0239,  0.1212,  0.0595],\n",
              "         [-0.0969,  0.1057,  0.0221,  ..., -0.0811, -0.1230, -0.0063],\n",
              "         [ 0.0978,  0.1072,  0.0771,  ..., -0.1022, -0.0793, -0.1164]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0876, -0.0638,  0.0228, -0.0719,  0.0782, -0.0517, -0.0864,  0.0277,\n",
              "          0.1223,  0.0892,  0.1079,  0.1104,  0.0378, -0.0874, -0.0584, -0.0302,\n",
              "         -0.1129, -0.1069, -0.0929,  0.0992, -0.0504, -0.0802, -0.0563, -0.0950,\n",
              "         -0.0707,  0.0378, -0.0589,  0.0443,  0.0945], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaUmur_GAtDn"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_dict={}\n",
        "def train(model,epochs,train_batch,valid_batch,lr,opt_fn=torch.optim.SGD):\n",
        "    opt = opt_fn(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        loss_dict[epoch] = 0\n",
        "        i = 0\n",
        "        for input_part, output_part in train_batch:\n",
        "            i+=1\n",
        "            output = model(input_part)\n",
        "            loss = F.mse_loss(output,output_part)\n",
        "            loss_dict[epoch]+=loss\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            #print(\"Done with {0} part of {1}/{2}\".format(i,epoch,epochs))\n",
        "        loss_dict[epoch]/1514\n",
        "        print(\"For epoch {0} avg_loss = {1}\".format(epoch,loss_dict[epoch]))\n",
        "    return loss_dict"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ttFKdDCDAtDr",
        "outputId": "d38ca0c7-4095-4600-e1d0-a527599e6afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 250\n",
        "lr = 1e-2\n",
        "\n",
        "history = train(model, epochs, train_loader, val_loader, lr)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "For epoch 0 avg_loss = 49.93855667114258\n",
            "For epoch 1 avg_loss = 44.1098518371582\n",
            "For epoch 2 avg_loss = 40.83225631713867\n",
            "For epoch 3 avg_loss = 36.78837966918945\n",
            "For epoch 4 avg_loss = 34.20107650756836\n",
            "For epoch 5 avg_loss = 32.32714080810547\n",
            "For epoch 6 avg_loss = 30.923852920532227\n",
            "For epoch 7 avg_loss = 29.813690185546875\n",
            "For epoch 8 avg_loss = 28.932092666625977\n",
            "For epoch 9 avg_loss = 28.185344696044922\n",
            "For epoch 10 avg_loss = 27.519119262695312\n",
            "For epoch 11 avg_loss = 26.92926597595215\n",
            "For epoch 12 avg_loss = 26.419239044189453\n",
            "For epoch 13 avg_loss = 25.960643768310547\n",
            "For epoch 14 avg_loss = 25.542551040649414\n",
            "For epoch 15 avg_loss = 25.143346786499023\n",
            "For epoch 16 avg_loss = 24.758968353271484\n",
            "For epoch 17 avg_loss = 24.377010345458984\n",
            "For epoch 18 avg_loss = 23.991470336914062\n",
            "For epoch 19 avg_loss = 23.603439331054688\n",
            "For epoch 20 avg_loss = 23.148340225219727\n",
            "For epoch 21 avg_loss = 21.144739151000977\n",
            "For epoch 22 avg_loss = 19.191011428833008\n",
            "For epoch 23 avg_loss = 17.5216064453125\n",
            "For epoch 24 avg_loss = 17.040246963500977\n",
            "For epoch 25 avg_loss = 16.713661193847656\n",
            "For epoch 26 avg_loss = 16.44392204284668\n",
            "For epoch 27 avg_loss = 16.2000789642334\n",
            "For epoch 28 avg_loss = 15.97806167602539\n",
            "For epoch 29 avg_loss = 15.780848503112793\n",
            "For epoch 30 avg_loss = 15.609854698181152\n",
            "For epoch 31 avg_loss = 15.46164608001709\n",
            "For epoch 32 avg_loss = 15.335530281066895\n",
            "For epoch 33 avg_loss = 15.221856117248535\n",
            "For epoch 34 avg_loss = 15.12109661102295\n",
            "For epoch 35 avg_loss = 15.029524803161621\n",
            "For epoch 36 avg_loss = 14.932403564453125\n",
            "For epoch 37 avg_loss = 14.804410934448242\n",
            "For epoch 38 avg_loss = 14.692792892456055\n",
            "For epoch 39 avg_loss = 14.528853416442871\n",
            "For epoch 40 avg_loss = 13.137457847595215\n",
            "For epoch 41 avg_loss = 12.762864112854004\n",
            "For epoch 42 avg_loss = 12.642101287841797\n",
            "For epoch 43 avg_loss = 12.56075382232666\n",
            "For epoch 44 avg_loss = 12.493303298950195\n",
            "For epoch 45 avg_loss = 12.43935775756836\n",
            "For epoch 46 avg_loss = 12.389281272888184\n",
            "For epoch 47 avg_loss = 12.324158668518066\n",
            "For epoch 48 avg_loss = 11.587751388549805\n",
            "For epoch 49 avg_loss = 11.252642631530762\n",
            "For epoch 50 avg_loss = 11.100607872009277\n",
            "For epoch 51 avg_loss = 11.01585578918457\n",
            "For epoch 52 avg_loss = 10.956467628479004\n",
            "For epoch 53 avg_loss = 10.907222747802734\n",
            "For epoch 54 avg_loss = 10.864984512329102\n",
            "For epoch 55 avg_loss = 10.824231147766113\n",
            "For epoch 56 avg_loss = 10.782947540283203\n",
            "For epoch 57 avg_loss = 10.74345588684082\n",
            "For epoch 58 avg_loss = 10.710140228271484\n",
            "For epoch 59 avg_loss = 10.68558406829834\n",
            "For epoch 60 avg_loss = 10.661901473999023\n",
            "For epoch 61 avg_loss = 10.642159461975098\n",
            "For epoch 62 avg_loss = 10.622573852539062\n",
            "For epoch 63 avg_loss = 10.606731414794922\n",
            "For epoch 64 avg_loss = 10.589106559753418\n",
            "For epoch 65 avg_loss = 10.572250366210938\n",
            "For epoch 66 avg_loss = 10.557500839233398\n",
            "For epoch 67 avg_loss = 10.544096946716309\n",
            "For epoch 68 avg_loss = 10.527793884277344\n",
            "For epoch 69 avg_loss = 10.515763282775879\n",
            "For epoch 70 avg_loss = 10.502904891967773\n",
            "For epoch 71 avg_loss = 10.49140739440918\n",
            "For epoch 72 avg_loss = 10.479926109313965\n",
            "For epoch 73 avg_loss = 10.466704368591309\n",
            "For epoch 74 avg_loss = 10.457910537719727\n",
            "For epoch 75 avg_loss = 10.446346282958984\n",
            "For epoch 76 avg_loss = 10.434889793395996\n",
            "For epoch 77 avg_loss = 10.425294876098633\n",
            "For epoch 78 avg_loss = 10.415938377380371\n",
            "For epoch 79 avg_loss = 10.409045219421387\n",
            "For epoch 80 avg_loss = 10.398977279663086\n",
            "For epoch 81 avg_loss = 10.38896656036377\n",
            "For epoch 82 avg_loss = 10.381958961486816\n",
            "For epoch 83 avg_loss = 10.375540733337402\n",
            "For epoch 84 avg_loss = 10.367026329040527\n",
            "For epoch 85 avg_loss = 10.358772277832031\n",
            "For epoch 86 avg_loss = 10.351158142089844\n",
            "For epoch 87 avg_loss = 10.344094276428223\n",
            "For epoch 88 avg_loss = 10.338403701782227\n",
            "For epoch 89 avg_loss = 10.332502365112305\n",
            "For epoch 90 avg_loss = 10.324322700500488\n",
            "For epoch 91 avg_loss = 10.316593170166016\n",
            "For epoch 92 avg_loss = 10.313963890075684\n",
            "For epoch 93 avg_loss = 10.305338859558105\n",
            "For epoch 94 avg_loss = 10.300543785095215\n",
            "For epoch 95 avg_loss = 10.29438591003418\n",
            "For epoch 96 avg_loss = 10.287345886230469\n",
            "For epoch 97 avg_loss = 10.282443046569824\n",
            "For epoch 98 avg_loss = 10.277328491210938\n",
            "For epoch 99 avg_loss = 10.270267486572266\n",
            "For epoch 100 avg_loss = 10.266141891479492\n",
            "For epoch 101 avg_loss = 10.261441230773926\n",
            "For epoch 102 avg_loss = 10.255687713623047\n",
            "For epoch 103 avg_loss = 10.24984359741211\n",
            "For epoch 104 avg_loss = 10.243183135986328\n",
            "For epoch 105 avg_loss = 10.238227844238281\n",
            "For epoch 106 avg_loss = 10.232475280761719\n",
            "For epoch 107 avg_loss = 10.227250099182129\n",
            "For epoch 108 avg_loss = 10.223739624023438\n",
            "For epoch 109 avg_loss = 10.217801094055176\n",
            "For epoch 110 avg_loss = 10.213021278381348\n",
            "For epoch 111 avg_loss = 10.208203315734863\n",
            "For epoch 112 avg_loss = 10.203402519226074\n",
            "For epoch 113 avg_loss = 10.19882583618164\n",
            "For epoch 114 avg_loss = 10.191691398620605\n",
            "For epoch 115 avg_loss = 10.188308715820312\n",
            "For epoch 116 avg_loss = 10.182833671569824\n",
            "For epoch 117 avg_loss = 10.177985191345215\n",
            "For epoch 118 avg_loss = 10.173173904418945\n",
            "For epoch 119 avg_loss = 10.168168067932129\n",
            "For epoch 120 avg_loss = 10.163726806640625\n",
            "For epoch 121 avg_loss = 10.15908432006836\n",
            "For epoch 122 avg_loss = 10.154356002807617\n",
            "For epoch 123 avg_loss = 10.149683952331543\n",
            "For epoch 124 avg_loss = 10.145984649658203\n",
            "For epoch 125 avg_loss = 10.142555236816406\n",
            "For epoch 126 avg_loss = 10.13862133026123\n",
            "For epoch 127 avg_loss = 10.133207321166992\n",
            "For epoch 128 avg_loss = 10.128641128540039\n",
            "For epoch 129 avg_loss = 10.125890731811523\n",
            "For epoch 130 avg_loss = 10.120292663574219\n",
            "For epoch 131 avg_loss = 10.117072105407715\n",
            "For epoch 132 avg_loss = 10.113390922546387\n",
            "For epoch 133 avg_loss = 10.109551429748535\n",
            "For epoch 134 avg_loss = 10.106060981750488\n",
            "For epoch 135 avg_loss = 10.102270126342773\n",
            "For epoch 136 avg_loss = 10.098800659179688\n",
            "For epoch 137 avg_loss = 10.094780921936035\n",
            "For epoch 138 avg_loss = 10.0939302444458\n",
            "For epoch 139 avg_loss = 10.088982582092285\n",
            "For epoch 140 avg_loss = 10.084423065185547\n",
            "For epoch 141 avg_loss = 10.08011245727539\n",
            "For epoch 142 avg_loss = 10.077350616455078\n",
            "For epoch 143 avg_loss = 10.073877334594727\n",
            "For epoch 144 avg_loss = 10.068536758422852\n",
            "For epoch 145 avg_loss = 10.06104564666748\n",
            "For epoch 146 avg_loss = 10.045119285583496\n",
            "For epoch 147 avg_loss = 9.990312576293945\n",
            "For epoch 148 avg_loss = 9.937589645385742\n",
            "For epoch 149 avg_loss = 9.828388214111328\n",
            "For epoch 150 avg_loss = 9.631392478942871\n",
            "For epoch 151 avg_loss = 9.417521476745605\n",
            "For epoch 152 avg_loss = 9.261197090148926\n",
            "For epoch 153 avg_loss = 9.160938262939453\n",
            "For epoch 154 avg_loss = 9.096633911132812\n",
            "For epoch 155 avg_loss = 9.05316162109375\n",
            "For epoch 156 avg_loss = 9.020903587341309\n",
            "For epoch 157 avg_loss = 8.999537467956543\n",
            "For epoch 158 avg_loss = 8.98037052154541\n",
            "For epoch 159 avg_loss = 8.965808868408203\n",
            "For epoch 160 avg_loss = 8.953226089477539\n",
            "For epoch 161 avg_loss = 8.942760467529297\n",
            "For epoch 162 avg_loss = 8.934492111206055\n",
            "For epoch 163 avg_loss = 8.925350189208984\n",
            "For epoch 164 avg_loss = 8.91932487487793\n",
            "For epoch 165 avg_loss = 8.911181449890137\n",
            "For epoch 166 avg_loss = 8.906328201293945\n",
            "For epoch 167 avg_loss = 8.897965431213379\n",
            "For epoch 168 avg_loss = 8.893310546875\n",
            "For epoch 169 avg_loss = 8.889339447021484\n",
            "For epoch 170 avg_loss = 8.883759498596191\n",
            "For epoch 171 avg_loss = 8.880764961242676\n",
            "For epoch 172 avg_loss = 8.875670433044434\n",
            "For epoch 173 avg_loss = 8.869894027709961\n",
            "For epoch 174 avg_loss = 8.86630916595459\n",
            "For epoch 175 avg_loss = 8.861869812011719\n",
            "For epoch 176 avg_loss = 8.85842514038086\n",
            "For epoch 177 avg_loss = 8.856886863708496\n",
            "For epoch 178 avg_loss = 8.85189437866211\n",
            "For epoch 179 avg_loss = 8.848895072937012\n",
            "For epoch 180 avg_loss = 8.846461296081543\n",
            "For epoch 181 avg_loss = 8.842222213745117\n",
            "For epoch 182 avg_loss = 8.83885383605957\n",
            "For epoch 183 avg_loss = 8.836347579956055\n",
            "For epoch 184 avg_loss = 8.833622932434082\n",
            "For epoch 185 avg_loss = 8.829185485839844\n",
            "For epoch 186 avg_loss = 8.826164245605469\n",
            "For epoch 187 avg_loss = 8.82272720336914\n",
            "For epoch 188 avg_loss = 8.821738243103027\n",
            "For epoch 189 avg_loss = 8.81862735748291\n",
            "For epoch 190 avg_loss = 8.818681716918945\n",
            "For epoch 191 avg_loss = 8.813339233398438\n",
            "For epoch 192 avg_loss = 8.81121826171875\n",
            "For epoch 193 avg_loss = 8.809877395629883\n",
            "For epoch 194 avg_loss = 8.80642318725586\n",
            "For epoch 195 avg_loss = 8.803722381591797\n",
            "For epoch 196 avg_loss = 8.803754806518555\n",
            "For epoch 197 avg_loss = 8.800337791442871\n",
            "For epoch 198 avg_loss = 8.797073364257812\n",
            "For epoch 199 avg_loss = 8.796137809753418\n",
            "For epoch 200 avg_loss = 8.793877601623535\n",
            "For epoch 201 avg_loss = 8.79024600982666\n",
            "For epoch 202 avg_loss = 8.788798332214355\n",
            "For epoch 203 avg_loss = 8.786623001098633\n",
            "For epoch 204 avg_loss = 8.783960342407227\n",
            "For epoch 205 avg_loss = 8.783061027526855\n",
            "For epoch 206 avg_loss = 8.780533790588379\n",
            "For epoch 207 avg_loss = 8.77782154083252\n",
            "For epoch 208 avg_loss = 8.776549339294434\n",
            "For epoch 209 avg_loss = 8.774238586425781\n",
            "For epoch 210 avg_loss = 8.773923873901367\n",
            "For epoch 211 avg_loss = 8.770160675048828\n",
            "For epoch 212 avg_loss = 8.769538879394531\n",
            "For epoch 213 avg_loss = 8.768146514892578\n",
            "For epoch 214 avg_loss = 8.766359329223633\n",
            "For epoch 215 avg_loss = 8.765212059020996\n",
            "For epoch 216 avg_loss = 8.76264762878418\n",
            "For epoch 217 avg_loss = 8.760342597961426\n",
            "For epoch 218 avg_loss = 8.760013580322266\n",
            "For epoch 219 avg_loss = 8.756421089172363\n",
            "For epoch 220 avg_loss = 8.752853393554688\n",
            "For epoch 221 avg_loss = 8.752358436584473\n",
            "For epoch 222 avg_loss = 8.75022029876709\n",
            "For epoch 223 avg_loss = 8.748835563659668\n",
            "For epoch 224 avg_loss = 8.746338844299316\n",
            "For epoch 225 avg_loss = 8.745966911315918\n",
            "For epoch 226 avg_loss = 8.744115829467773\n",
            "For epoch 227 avg_loss = 8.74104118347168\n",
            "For epoch 228 avg_loss = 8.74051570892334\n",
            "For epoch 229 avg_loss = 8.738673210144043\n",
            "For epoch 230 avg_loss = 8.736953735351562\n",
            "For epoch 231 avg_loss = 8.734742164611816\n",
            "For epoch 232 avg_loss = 8.734278678894043\n",
            "For epoch 233 avg_loss = 8.732121467590332\n",
            "For epoch 234 avg_loss = 8.730914115905762\n",
            "For epoch 235 avg_loss = 8.728957176208496\n",
            "For epoch 236 avg_loss = 8.727788925170898\n",
            "For epoch 237 avg_loss = 8.726950645446777\n",
            "For epoch 238 avg_loss = 8.725301742553711\n",
            "For epoch 239 avg_loss = 8.722406387329102\n",
            "For epoch 240 avg_loss = 8.721288681030273\n",
            "For epoch 241 avg_loss = 8.719305038452148\n",
            "For epoch 242 avg_loss = 8.718706130981445\n",
            "For epoch 243 avg_loss = 8.71514892578125\n",
            "For epoch 244 avg_loss = 8.715811729431152\n",
            "For epoch 245 avg_loss = 8.71372127532959\n",
            "For epoch 246 avg_loss = 8.711954116821289\n",
            "For epoch 247 avg_loss = 8.711529731750488\n",
            "For epoch 248 avg_loss = 8.709057807922363\n",
            "For epoch 249 avg_loss = 8.708106994628906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWma7v9_AtDw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDL2wjEbAtDz",
        "outputId": "226c79d7-d16d-4122-f369-75dcd0f5a97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(list(loss_dict.keys()), list(loss_dict.values()))\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R31dBBL9AtD3"
      },
      "source": [
        "torch.save(model.state_dict(), 'crop_prediction_weights_final_best_trained.pth')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgHYctDYAtD6",
        "outputId": "53a3eced-a412-48ac-972d-701c713ae8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('input.weight',\n",
              "              tensor([[ 0.3404,  0.1901, -0.0578, -0.2643, -0.2768,  0.0169,  0.0388,  0.2357],\n",
              "                      [ 0.1347,  0.2948, -0.3193, -0.1689,  0.1443,  0.3682,  0.4275, -0.2829],\n",
              "                      [ 0.2704, -0.4523,  0.0994,  0.1274,  0.5328,  0.1060, -0.3626, -0.0180],\n",
              "                      [ 0.3134, -0.2717, -0.0144,  0.4164, -0.0395, -0.0031, -0.2717, -0.2886],\n",
              "                      [-0.2924,  0.0481,  0.1436,  0.3414,  0.1725,  0.5080,  0.3470,  0.4032],\n",
              "                      [-0.1258, -0.1319,  0.3874, -0.2225, -0.2533, -0.2674, -0.4112, -0.3357],\n",
              "                      [ 0.2865,  0.3494, -0.0064,  0.1623, -0.2691,  0.2427,  0.4432, -0.4812],\n",
              "                      [ 0.1907, -0.0162,  0.0058, -0.0231, -0.2883, -0.0731, -0.1720, -0.1836],\n",
              "                      [ 0.3598,  0.0365,  0.3593, -0.0399,  0.4266, -0.0385,  0.1772,  0.5515],\n",
              "                      [-0.0342, -0.3048, -0.1325, -0.1461,  0.0543, -0.1569,  0.0463,  0.1447],\n",
              "                      [-0.2218, -0.2505, -0.0974, -0.2080, -0.2442,  0.2944, -0.1895,  0.0076],\n",
              "                      [ 0.0446, -0.2094, -0.0657, -0.0116, -0.3317, -0.2069, -0.0514,  0.0663],\n",
              "                      [-0.3057,  0.0137, -0.1135,  0.1273, -0.0445,  0.1503, -0.2536, -0.0665],\n",
              "                      [-0.2874, -0.1059,  0.3725, -0.3632,  0.0540,  0.3535, -0.3011,  0.0431],\n",
              "                      [-0.2927, -0.0067, -0.0695, -0.0038, -0.2952,  0.5073, -0.0271,  0.3089],\n",
              "                      [ 0.2126, -0.0067,  0.2039, -0.2527,  0.3892,  0.3001, -0.5252, -0.6586],\n",
              "                      [ 0.0677, -0.3342, -0.0378, -0.4229,  0.2380,  0.5113, -0.1616, -0.0914],\n",
              "                      [ 0.1064, -0.2767, -0.2523, -0.2168, -0.2096, -0.3482, -0.2864,  0.1574],\n",
              "                      [-0.2275,  0.0668, -0.1443, -0.0153, -0.1469,  0.4883,  0.1884,  0.1002],\n",
              "                      [ 0.0243,  0.2253, -0.1216, -0.2713, -0.3887,  0.4225, -0.1192,  0.2126],\n",
              "                      [-0.1014, -0.0413, -0.1663,  0.0715, -0.3226, -0.1905, -0.2089,  0.0261],\n",
              "                      [ 0.3880,  0.0304,  0.2764,  0.3618, -0.5520, -0.0309,  0.1234, -0.4155],\n",
              "                      [-0.0167,  0.3440,  0.3737,  0.1906, -0.5138, -0.1158, -0.2904,  0.5050],\n",
              "                      [-0.1745,  0.0114,  0.5195, -0.2818,  0.3256, -0.6529, -0.1057, -0.1012],\n",
              "                      [-0.2380, -0.1868,  0.4799,  0.0071, -0.4008,  0.2031,  0.1409, -0.3806],\n",
              "                      [-0.2861, -0.3458,  0.2776, -0.1344, -0.3608, -0.0616,  0.1595, -0.0912],\n",
              "                      [ 0.1138, -0.2399,  0.1980, -0.3639, -0.1360,  0.0362,  0.5794,  0.0647],\n",
              "                      [ 0.0502,  0.1861, -0.1691, -0.2302, -0.2101,  0.2281,  0.0721,  0.0305],\n",
              "                      [-0.2296, -0.1198,  0.0945,  0.0126, -0.3678,  0.6944, -0.4390, -0.0110],\n",
              "                      [-0.0795,  0.1716, -0.0292, -0.1114, -0.3499, -0.1532, -0.3376,  0.2099],\n",
              "                      [ 0.2545,  0.2835,  0.1892, -0.4262, -0.0441,  0.1970, -0.0356,  0.1984],\n",
              "                      [-0.1986,  0.1981,  0.2773, -0.3912, -0.1950, -0.1128,  0.0989, -0.1293]])),\n",
              "             ('input.bias',\n",
              "              tensor([ 0.2543, -0.1018, -0.2170,  0.2304, -0.0290, -0.1405,  0.0410, -0.1589,\n",
              "                       0.0229,  0.0372, -0.1687, -0.3001,  0.0127,  0.0979, -0.0415,  0.3445,\n",
              "                       0.0853,  0.3051, -0.3281, -0.1364,  0.1171, -0.1423, -0.1918,  0.3124,\n",
              "                       0.0536, -0.0840,  0.2934, -0.1965, -0.2279,  0.1933, -0.1025,  0.1447])),\n",
              "             ('hidden1.weight',\n",
              "              tensor([[ 0.1274,  0.0208, -0.2216,  ..., -0.1409, -0.0663, -0.1710],\n",
              "                      [-0.1669, -0.1171, -0.0280,  ...,  0.1472, -0.0885,  0.1697],\n",
              "                      [-0.0653,  0.0589, -0.1553,  ..., -0.0500,  0.1148, -0.1666],\n",
              "                      ...,\n",
              "                      [-0.1058,  0.1545, -0.0670,  ...,  0.0972,  0.1740,  0.0781],\n",
              "                      [ 0.0975,  0.0533,  0.2067,  ..., -0.0838, -0.1354, -0.0299],\n",
              "                      [ 0.1514, -0.0485, -0.1287,  ..., -0.0354,  0.1072, -0.1247]])),\n",
              "             ('hidden1.bias',\n",
              "              tensor([ 0.1244,  0.0716,  0.1133,  0.0672, -0.0201,  0.1042,  0.1131, -0.0157,\n",
              "                       0.1741, -0.0009, -0.1128, -0.0579, -0.0181, -0.0969, -0.1052,  0.1011,\n",
              "                       0.0246, -0.0442, -0.1561,  0.0862,  0.0433, -0.0035,  0.0903, -0.1577,\n",
              "                      -0.0738,  0.0361,  0.1298, -0.1300, -0.0603,  0.1498, -0.0139, -0.1290,\n",
              "                       0.0941, -0.0352, -0.0723,  0.1505, -0.1552,  0.1449,  0.0723, -0.0139,\n",
              "                      -0.0276,  0.0108,  0.1637,  0.0145, -0.0189,  0.0778,  0.0849,  0.1736,\n",
              "                       0.0135, -0.1033, -0.0781,  0.0751, -0.1059, -0.1394,  0.1438,  0.1539,\n",
              "                      -0.1494,  0.1948, -0.1531,  0.0395, -0.0891, -0.0154, -0.1547, -0.0166])),\n",
              "             ('hidden2.weight',\n",
              "              tensor([[-0.0094, -0.0656, -0.0718,  ..., -0.1443,  0.0706,  0.0188],\n",
              "                      [-0.0920,  0.1070,  0.1433,  ..., -0.1489,  0.0653, -0.0401],\n",
              "                      [-0.0702,  0.0940, -0.1964,  ...,  0.1484, -0.1068,  0.0070],\n",
              "                      ...,\n",
              "                      [ 0.0963,  0.0918,  0.0479,  ..., -0.0608,  0.0995,  0.0595],\n",
              "                      [-0.0969,  0.1057,  0.0221,  ..., -0.0811, -0.1229, -0.0063],\n",
              "                      [ 0.1454,  0.1065,  0.1505,  ..., -0.0588, -0.0846, -0.1164]])),\n",
              "             ('hidden2.bias',\n",
              "              tensor([-0.0907, -0.0630,  0.0284, -0.0564,  0.0830, -0.0514, -0.1397,  0.0340,\n",
              "                       0.1223,  0.0895,  0.1232,  0.1295,  0.0552, -0.1281, -0.0651, -0.0069,\n",
              "                      -0.1780, -0.1035, -0.0928,  0.0941, -0.0422, -0.0567, -0.0428, -0.0866,\n",
              "                      -0.0770,  0.0364, -0.0608,  0.0443,  0.1118]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSP-rBheAtD9",
        "outputId": "0fcbfe28-0898-4444-e9f3-80e6877ac7b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model2 = Model()\n",
        "model2.load_state_dict(torch.load('crop_prediction_weights_final_best_trained.pth'))\n",
        "model2.state_dict()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('input.weight',\n",
              "              tensor([[ 0.3404,  0.1901, -0.0578, -0.2643, -0.2768,  0.0169,  0.0388,  0.2357],\n",
              "                      [ 0.1347,  0.2948, -0.3193, -0.1689,  0.1443,  0.3682,  0.4275, -0.2829],\n",
              "                      [ 0.2704, -0.4523,  0.0994,  0.1274,  0.5328,  0.1060, -0.3626, -0.0180],\n",
              "                      [ 0.3134, -0.2717, -0.0144,  0.4164, -0.0395, -0.0031, -0.2717, -0.2886],\n",
              "                      [-0.2924,  0.0481,  0.1436,  0.3414,  0.1725,  0.5080,  0.3470,  0.4032],\n",
              "                      [-0.1258, -0.1319,  0.3874, -0.2225, -0.2533, -0.2674, -0.4112, -0.3357],\n",
              "                      [ 0.2865,  0.3494, -0.0064,  0.1623, -0.2691,  0.2427,  0.4432, -0.4812],\n",
              "                      [ 0.1907, -0.0162,  0.0058, -0.0231, -0.2883, -0.0731, -0.1720, -0.1836],\n",
              "                      [ 0.3598,  0.0365,  0.3593, -0.0399,  0.4266, -0.0385,  0.1772,  0.5515],\n",
              "                      [-0.0342, -0.3048, -0.1325, -0.1461,  0.0543, -0.1569,  0.0463,  0.1447],\n",
              "                      [-0.2218, -0.2505, -0.0974, -0.2080, -0.2442,  0.2944, -0.1895,  0.0076],\n",
              "                      [ 0.0446, -0.2094, -0.0657, -0.0116, -0.3317, -0.2069, -0.0514,  0.0663],\n",
              "                      [-0.3057,  0.0137, -0.1135,  0.1273, -0.0445,  0.1503, -0.2536, -0.0665],\n",
              "                      [-0.2874, -0.1059,  0.3725, -0.3632,  0.0540,  0.3535, -0.3011,  0.0431],\n",
              "                      [-0.2927, -0.0067, -0.0695, -0.0038, -0.2952,  0.5073, -0.0271,  0.3089],\n",
              "                      [ 0.2126, -0.0067,  0.2039, -0.2527,  0.3892,  0.3001, -0.5252, -0.6586],\n",
              "                      [ 0.0677, -0.3342, -0.0378, -0.4229,  0.2380,  0.5113, -0.1616, -0.0914],\n",
              "                      [ 0.1064, -0.2767, -0.2523, -0.2168, -0.2096, -0.3482, -0.2864,  0.1574],\n",
              "                      [-0.2275,  0.0668, -0.1443, -0.0153, -0.1469,  0.4883,  0.1884,  0.1002],\n",
              "                      [ 0.0243,  0.2253, -0.1216, -0.2713, -0.3887,  0.4225, -0.1192,  0.2126],\n",
              "                      [-0.1014, -0.0413, -0.1663,  0.0715, -0.3226, -0.1905, -0.2089,  0.0261],\n",
              "                      [ 0.3880,  0.0304,  0.2764,  0.3618, -0.5520, -0.0309,  0.1234, -0.4155],\n",
              "                      [-0.0167,  0.3440,  0.3737,  0.1906, -0.5138, -0.1158, -0.2904,  0.5050],\n",
              "                      [-0.1745,  0.0114,  0.5195, -0.2818,  0.3256, -0.6529, -0.1057, -0.1012],\n",
              "                      [-0.2380, -0.1868,  0.4799,  0.0071, -0.4008,  0.2031,  0.1409, -0.3806],\n",
              "                      [-0.2861, -0.3458,  0.2776, -0.1344, -0.3608, -0.0616,  0.1595, -0.0912],\n",
              "                      [ 0.1138, -0.2399,  0.1980, -0.3639, -0.1360,  0.0362,  0.5794,  0.0647],\n",
              "                      [ 0.0502,  0.1861, -0.1691, -0.2302, -0.2101,  0.2281,  0.0721,  0.0305],\n",
              "                      [-0.2296, -0.1198,  0.0945,  0.0126, -0.3678,  0.6944, -0.4390, -0.0110],\n",
              "                      [-0.0795,  0.1716, -0.0292, -0.1114, -0.3499, -0.1532, -0.3376,  0.2099],\n",
              "                      [ 0.2545,  0.2835,  0.1892, -0.4262, -0.0441,  0.1970, -0.0356,  0.1984],\n",
              "                      [-0.1986,  0.1981,  0.2773, -0.3912, -0.1950, -0.1128,  0.0989, -0.1293]])),\n",
              "             ('input.bias',\n",
              "              tensor([ 0.2543, -0.1018, -0.2170,  0.2304, -0.0290, -0.1405,  0.0410, -0.1589,\n",
              "                       0.0229,  0.0372, -0.1687, -0.3001,  0.0127,  0.0979, -0.0415,  0.3445,\n",
              "                       0.0853,  0.3051, -0.3281, -0.1364,  0.1171, -0.1423, -0.1918,  0.3124,\n",
              "                       0.0536, -0.0840,  0.2934, -0.1965, -0.2279,  0.1933, -0.1025,  0.1447])),\n",
              "             ('hidden1.weight',\n",
              "              tensor([[ 0.1274,  0.0208, -0.2216,  ..., -0.1409, -0.0663, -0.1710],\n",
              "                      [-0.1669, -0.1171, -0.0280,  ...,  0.1472, -0.0885,  0.1697],\n",
              "                      [-0.0653,  0.0589, -0.1553,  ..., -0.0500,  0.1148, -0.1666],\n",
              "                      ...,\n",
              "                      [-0.1058,  0.1545, -0.0670,  ...,  0.0972,  0.1740,  0.0781],\n",
              "                      [ 0.0975,  0.0533,  0.2067,  ..., -0.0838, -0.1354, -0.0299],\n",
              "                      [ 0.1514, -0.0485, -0.1287,  ..., -0.0354,  0.1072, -0.1247]])),\n",
              "             ('hidden1.bias',\n",
              "              tensor([ 0.1244,  0.0716,  0.1133,  0.0672, -0.0201,  0.1042,  0.1131, -0.0157,\n",
              "                       0.1741, -0.0009, -0.1128, -0.0579, -0.0181, -0.0969, -0.1052,  0.1011,\n",
              "                       0.0246, -0.0442, -0.1561,  0.0862,  0.0433, -0.0035,  0.0903, -0.1577,\n",
              "                      -0.0738,  0.0361,  0.1298, -0.1300, -0.0603,  0.1498, -0.0139, -0.1290,\n",
              "                       0.0941, -0.0352, -0.0723,  0.1505, -0.1552,  0.1449,  0.0723, -0.0139,\n",
              "                      -0.0276,  0.0108,  0.1637,  0.0145, -0.0189,  0.0778,  0.0849,  0.1736,\n",
              "                       0.0135, -0.1033, -0.0781,  0.0751, -0.1059, -0.1394,  0.1438,  0.1539,\n",
              "                      -0.1494,  0.1948, -0.1531,  0.0395, -0.0891, -0.0154, -0.1547, -0.0166])),\n",
              "             ('hidden2.weight',\n",
              "              tensor([[-0.0094, -0.0656, -0.0718,  ..., -0.1443,  0.0706,  0.0188],\n",
              "                      [-0.0920,  0.1070,  0.1433,  ..., -0.1489,  0.0653, -0.0401],\n",
              "                      [-0.0702,  0.0940, -0.1964,  ...,  0.1484, -0.1068,  0.0070],\n",
              "                      ...,\n",
              "                      [ 0.0963,  0.0918,  0.0479,  ..., -0.0608,  0.0995,  0.0595],\n",
              "                      [-0.0969,  0.1057,  0.0221,  ..., -0.0811, -0.1229, -0.0063],\n",
              "                      [ 0.1454,  0.1065,  0.1505,  ..., -0.0588, -0.0846, -0.1164]])),\n",
              "             ('hidden2.bias',\n",
              "              tensor([-0.0907, -0.0630,  0.0284, -0.0564,  0.0830, -0.0514, -0.1397,  0.0340,\n",
              "                       0.1223,  0.0895,  0.1232,  0.1295,  0.0552, -0.1281, -0.0651, -0.0069,\n",
              "                      -0.1780, -0.1035, -0.0928,  0.0941, -0.0422, -0.0567, -0.0428, -0.0866,\n",
              "                      -0.0770,  0.0364, -0.0608,  0.0443,  0.1118]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoENE087tDOP",
        "outputId": "908f6737-63cf-4c21-9a08-fb1cdb24a927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for val in val_loader:\n",
        "  valid_ip, valid_op = val\n",
        "  pred_valid = model2(valid_ip)\n",
        "  for pred, origi in zip(pred_valid.detach().numpy(), valid_op.detach().numpy()):\n",
        "    i = np.argmax(pred)\n",
        "    j = np.argmax(origi)\n",
        "    total+=1\n",
        "    if (i==j):\n",
        "      correct+=1\n",
        "print(\"Accuracy = \", correct*100/total)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  84.89340439706862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOtWlnR2AtEA",
        "outputId": "5e510ee0-a5c6-417c-82b1-4a84a8234321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        }
      },
      "source": [
        "crops = ['arhar', 'bajra', 'barley', 'coriander', 'cotton', 'cowpea', 'dry chillies', 'garlic', 'ginger', 'gram', 'groundnut', 'jowar', 'linseed', 'maize-k', 'maize-r', 'masoor', 'moong', 'onion', 'peas&beans', 'potato', 'ragi', 'rapeseed', 'rice', 'safflower', 'sugarcane', 'sunflower', 'turmeric', 'urad', 'wheat']\n",
        "\n",
        "\n",
        "pred_user = model2(torch.from_numpy(np.array([[17, 6.5, 40, 20, 17, 75, 25, 25]], dtype='float32')))\n",
        "print(pred_user)\n",
        "\n",
        "greater_than_zero={}\n",
        "index = -1\n",
        "count = 0\n",
        "for i in pred_user:\n",
        "    for p in i:\n",
        "        index+=1\n",
        "        if(p > 0):\n",
        "            greater_than_zero[crops[index]] = p \n",
        "    for i in greater_than_zero:\n",
        "        print(\"Crop:- {0}  Probab:- {1}\".format(i,greater_than_zero[i]*100))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.4613e-11, 7.3618e-07, 6.0691e-10, 1.8440e-08, 3.0192e-13, 3.0173e-14,\n",
            "         2.4937e-20, 3.5036e-10, 1.5884e-17, 8.6209e-16, 3.0756e-16, 3.6355e-07,\n",
            "         5.4691e-12, 1.4335e-10, 1.3859e-03, 9.4492e-20, 6.0366e-29, 2.6463e-05,\n",
            "         2.1942e-16, 9.9616e-01, 3.3612e-07, 1.3281e-26, 2.3006e-03, 1.0969e-17,\n",
            "         2.1042e-06, 9.0081e-16, 7.0373e-09, 1.4764e-19, 1.2164e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "Crop:- arhar  Probab:- 2.4613475702039977e-09\n",
            "Crop:- bajra  Probab:- 7.361821917584166e-05\n",
            "Crop:- barley  Probab:- 6.069130620289798e-08\n",
            "Crop:- coriander  Probab:- 1.8440263147567748e-06\n",
            "Crop:- cotton  Probab:- 3.019167901796571e-11\n",
            "Crop:- cowpea  Probab:- 3.017278050673755e-12\n",
            "Crop:- dry chillies  Probab:- 2.493667685053651e-18\n",
            "Crop:- garlic  Probab:- 3.503550871641892e-08\n",
            "Crop:- ginger  Probab:- 1.588397052030969e-15\n",
            "Crop:- gram  Probab:- 8.620898726873286e-14\n",
            "Crop:- groundnut  Probab:- 3.075623726512232e-14\n",
            "Crop:- jowar  Probab:- 3.635514076449908e-05\n",
            "Crop:- linseed  Probab:- 5.469084629616816e-10\n",
            "Crop:- maize-k  Probab:- 1.4334839981700043e-08\n",
            "Crop:- maize-r  Probab:- 0.13858678936958313\n",
            "Crop:- masoor  Probab:- 9.449243133579083e-18\n",
            "Crop:- moong  Probab:- 6.0366432917791074e-27\n",
            "Crop:- onion  Probab:- 0.0026463335379958153\n",
            "Crop:- peas&beans  Probab:- 2.1942205539506043e-14\n",
            "Crop:- potato  Probab:- 99.61619567871094\n",
            "Crop:- ragi  Probab:- 3.361159542691894e-05\n",
            "Crop:- rapeseed  Probab:- 1.328101370911581e-24\n",
            "Crop:- rice  Probab:- 0.2300565391778946\n",
            "Crop:- safflower  Probab:- 1.0968602328492727e-15\n",
            "Crop:- sugarcane  Probab:- 0.0002104155282722786\n",
            "Crop:- sunflower  Probab:- 9.00812559853463e-14\n",
            "Crop:- turmeric  Probab:- 7.037257887532178e-07\n",
            "Crop:- urad  Probab:- 1.4764075438218074e-17\n",
            "Crop:- wheat  Probab:- 0.012163884937763214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA5P9oSsAtEF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnLAbl-lAtEJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}